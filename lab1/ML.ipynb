{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec0f43",
   "metadata": {},
   "source": [
    "# задача 1) разведочный анализ данных (EDA): построить графики зависимости некоторых признаков друг от друга, график целевой переменной и матрицу корреляций, сделать выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb4a13",
   "metadata": {},
   "source": [
    "## Информация о данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train.info())\n",
    "missing_train = train.isnull().sum()\n",
    "missing_percent = 100 * missing_train / len(train)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Признак': missing_train.index,\n",
    "    'Пропусков': missing_train.values,\n",
    "    'Процент': missing_percent.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Пропусков'] > 0].sort_values('Пропусков', ascending=False)\n",
    "if len(missing_df) > 0:\n",
    "    # print(missing_df)\n",
    "    print(f\"\\n  Пропуски в {len(missing_df)} признаках:\")\n",
    "    print(missing_df.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"Пропущенных значений нет\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41393c43",
   "metadata": {},
   "source": [
    "#### Микровывод:\n",
    "Пропуски в 35 признаках (80% от всех). \n",
    "Две группы:\n",
    "- 9 признаков: 9.36% пропусков (финансовые и кредитные показатели)\n",
    "- 26 признаков: 4.81% пропусков (включая целевую переменную RiskScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852d8ad",
   "metadata": {},
   "source": [
    "## Статистическое описание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.describe().T)\n",
    "categorical_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nКатегориальные признаки: {categorical_cols}\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(train[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8b3d1",
   "metadata": {},
   "source": [
    "#### Микровывод:\n",
    "\n",
    "**Числовые признаки** (29 шт.):\n",
    "- Критичная проблема: RiskScore содержит экстремальные значения от **-10M до +10M** (среднее = -25,698), что указывает на **аномалии/ошибки ввода данных**\n",
    "- Финансовые метрики имеют большой разброс: TotalAssets (892 - 11M), AnnualIncome (15K - 748K)\n",
    "- Демографические признаки в нормальных диапазонах: Age (18-80), Experience (0-57)\n",
    "\n",
    "**Категориальные признаки** (6 шт.):\n",
    "- ApplicationDate: 10,000 уникальных дат - признак непригоден для прямого использования (требуется feature engineering: год, месяц, день недели)\n",
    "- Остальные признаки сбалансированы:\n",
    "  - MaritalStatus: Married (49%), Single (31%), Divorced (15%), Widowed (5%)\n",
    "  - EmploymentStatus: Employed (86%), Self-Employed (9%), Unemployed (4%)\n",
    "  - EducationLevel: относительно равномерное распределение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5df6e",
   "metadata": {},
   "source": [
    "## Анализ целевой переменной RISKSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Среднее: {train['RiskScore'].mean():.2f}\")\n",
    "print(f\"Медиана: {train['RiskScore'].median():.2f}\")\n",
    "print(f\"Стандартное отклонение: {train['RiskScore'].std():.2f}\")\n",
    "print(f\"Минимум: {train['RiskScore'].min():.2f}\")\n",
    "print(f\"Максимум: {train['RiskScore'].max():.2f}\")\n",
    "print(f\"Асимметрия (skewness): {train['RiskScore'].skew():.2f}\")\n",
    "print(f\"Эксцесс (kurtosis): {train['RiskScore'].kurtosis():.2f}\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Гистограмма\n",
    "axes[0].hist(train['RiskScore'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('RiskScore', fontsize=12)\n",
    "axes[0].set_ylabel('Частота', fontsize=12)\n",
    "axes[0].set_title('Распределение RiskScore', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(train['RiskScore'].mean(), color='red', linestyle='--', \n",
    "                label=f'Среднее: {train[\"RiskScore\"].mean():.2f}')\n",
    "axes[0].axvline(train['RiskScore'].median(), color='green', linestyle='--', \n",
    "                label=f'Медиана: {train[\"RiskScore\"].median():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(train['RiskScore'], vert=True)\n",
    "axes[1].set_ylabel('RiskScore', fontsize=12)\n",
    "axes[1].set_title('Box Plot RiskScore', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7483b6",
   "metadata": {},
   "source": [
    "#### Микровывод:\n",
    "\n",
    "**RiskScore имеет критичные аномалии:**\n",
    "- Диапазон: -10M до +10M (явные placeholder-значения)\n",
    "- Среднее (-25,699) vs Медиана (44.12) → **выбросы искажают статистику**\n",
    "- Эксцесс 45.77 (норма ≈ 3) → **множественные выбросы**\n",
    "- Медиана: **44.12** (более реалистичное значение для риск-скора)\n",
    "\n",
    "\n",
    "**Действие:** Фильтровать значения вне диапазона 0-200. Медиана подсказывает, что большинство данных в нормальном диапазоне."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b220c",
   "metadata": {},
   "source": [
    "## Лютый корреляционный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f91d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlation_matrix = train[numeric_cols].corr()\n",
    "\n",
    "if 'RiskScore' in numeric_cols:\n",
    "    correlations = correlation_matrix['RiskScore'].sort_values(ascending=False)\n",
    "    print(\"\\nКорреляция всех признаков с RiskScore:\")\n",
    "    print(correlations)\n",
    "    \n",
    "    print(f\"Максимальная положительная корреляция: {correlations[1]:.4f} ({correlations.index[1]})\")\n",
    "    print(f\"Максимальная отрицательная корреляция: {correlations.iloc[-1]:.4f} ({correlations.index[-1]})\")\n",
    "    print(f\"Средняя абсолютная корреляция: {correlations.drop('RiskScore').abs().mean():.4f}\")\n",
    "    print(f\"Медианная абсолютная корреляция: {correlations.drop('RiskScore').abs().median():.4f}\")\n",
    "    \n",
    "    # Категоризация корреляций\n",
    "    strong_positive = correlations[(correlations > 0.1) & (correlations < 1.0)]\n",
    "    moderate_positive = correlations[(correlations > 0.05) & (correlations <= 0.1)]\n",
    "    weak_positive = correlations[(correlations > 0) & (correlations <= 0.05)]\n",
    "    weak_negative = correlations[(correlations < 0) & (correlations >= -0.05)]\n",
    "    moderate_negative = correlations[(correlations < -0.05) & (correlations >= -0.1)]\n",
    "    strong_negative = correlations[correlations < -0.1]\n",
    "    \n",
    "    print(f\"\\nРаспределение силы корреляций:\")\n",
    "    print(f\"  Сильная положительная (>0.1):       {len(strong_positive)} признаков\")\n",
    "    print(f\"  Умеренная положительная (0.05-0.1): {len(moderate_positive)} признаков\")\n",
    "    print(f\"  Слабая положительная (0-0.05):      {len(weak_positive)} признаков\")\n",
    "    print(f\"  Слабая отрицательная (-0.05-0):     {len(weak_negative)} признаков\")\n",
    "    print(f\"  Умеренная отрицательная (-0.1--0.05): {len(moderate_negative)} признаков\")\n",
    "    print(f\"  Сильная отрицательная (<-0.1):      {len(strong_negative)} признаков\")\n",
    "    \n",
    "    if len(strong_positive) > 0:\n",
    "        print(f\"\\nСИЛЬНО коррелированные признаки (>0.1):\")\n",
    "        for feat, corr in strong_positive.items():\n",
    "            print(f\"  {feat:40s}: {corr:.4f}\")\n",
    "    \n",
    "    if len(strong_negative) > 0:\n",
    "        print(f\"\\nСИЛЬНО отрицательно коррелированные признаки (<-0.1):\")\n",
    "        for feat, corr in strong_negative.items():\n",
    "            print(f\"  {feat:40s}: {corr:.4f}\")\n",
    "\n",
    "\n",
    "print(\"ПОЛНЫЙ АНАЛИЗ КОРРЕЛЯЦИЙ МЕЖДУ ВСЕМИ ПРИЗНАКАМИ\")\n",
    "all_pairs = []\n",
    "features = [col for col in numeric_cols if col != 'RiskScore']\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        corr_value = correlation_matrix.loc[features[i], features[j]]\n",
    "        all_pairs.append({\n",
    "            'Feature_1': features[i],\n",
    "            'Feature_2': features[j],\n",
    "            'Correlation': corr_value,\n",
    "            'Abs_Correlation': abs(corr_value)\n",
    "        })\n",
    "\n",
    "all_pairs_df = pd.DataFrame(all_pairs)\n",
    "\n",
    "print(f\"\\nОбщее количество уникальных пар признаков: {len(all_pairs_df)}\")\n",
    "\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "labels = ['0.0-0.1', '0.1-0.2', '0.2-0.3', '0.3-0.4', '0.4-0.5', \n",
    "          '0.5-0.6', '0.6-0.7', '0.7-0.8', '0.8-0.9', '0.9-1.0']\n",
    "\n",
    "all_pairs_df['Corr_Range'] = pd.cut(all_pairs_df['Abs_Correlation'], bins=bins, labels=labels)\n",
    "distribution = all_pairs_df['Corr_Range'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nРаспределение ВСЕХ парных корреляций по диапазонам:\")\n",
    "for range_label, count in distribution.items():\n",
    "    percentage = (count / len(all_pairs_df)) * 100\n",
    "    print(f\"  |r| в диапазоне {range_label}: {count:4d} пар ({percentage:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nДЕТАЛЬНАЯ СТАТИСТИКА:\")\n",
    "print(f\"  Минимальная абсолютная корреляция: {all_pairs_df['Abs_Correlation'].min():.6f}\")\n",
    "print(f\"  Максимальная абсолютная корреляция: {all_pairs_df['Abs_Correlation'].max():.6f}\")\n",
    "print(f\"  Средняя абсолютная корреляция:     {all_pairs_df['Abs_Correlation'].mean():.6f}\")\n",
    "print(f\"  Медианная абсолютная корреляция:   {all_pairs_df['Abs_Correlation'].median():.6f}\")\n",
    "print(f\"  Стд. отклонение:                   {all_pairs_df['Abs_Correlation'].std():.6f}\")\n",
    "\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(f\"\\nПерцентили абсолютных корреляций:\")\n",
    "for p in percentiles:\n",
    "    val = all_pairs_df['Abs_Correlation'].quantile(p/100)\n",
    "    print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    count = (all_pairs_df['Abs_Correlation'] > threshold).sum()\n",
    "    percentage = (count / len(all_pairs_df)) * 100\n",
    "    print(f\"  |r| > {threshold:.1f}: {count:4d} пар ({percentage:5.2f}%)\")\n",
    "\n",
    "top_corr = all_pairs_df.nlargest(20, 'Abs_Correlation')\n",
    "print(top_corr[['Feature_1', 'Feature_2', 'Correlation', 'Abs_Correlation']].to_string(index=False))\n",
    "\n",
    "top_positive = all_pairs_df.nlargest(15, 'Correlation')\n",
    "print(top_positive[['Feature_1', 'Feature_2', 'Correlation']].to_string(index=False))\n",
    "\n",
    "top_negative = all_pairs_df.nsmallest(15, 'Correlation')\n",
    "print(top_negative[['Feature_1', 'Feature_2', 'Correlation']].to_string(index=False))\n",
    "\n",
    "for threshold in [0.3, 0.5, 0.7, 0.9]:\n",
    "    high_corr = all_pairs_df[all_pairs_df['Abs_Correlation'] > threshold]\n",
    "    print(f\"\\nПары с |r| > {threshold}:\")\n",
    "    if len(high_corr) > 0:\n",
    "        print(high_corr[['Feature_1', 'Feature_2', 'Correlation', 'Abs_Correlation']].to_string(index=False))\n",
    "        \n",
    "        if threshold >= 0.9:\n",
    "            print(f\"\\n  КРИТИЧЕСКАЯ МУЛЬТИКОЛЛИНЕАРНОСТЬ!\")\n",
    "            print(f\"  Рекомендация: Удалить один из признаков в каждой паре\")\n",
    "    else:\n",
    "        print(f\"  Пар не найдено\")\n",
    "\n",
    "feature_groups = {\n",
    "    'Финансовые показатели': ['AnnualIncome', 'MonthlyIncome', 'SavingsAccountBalance', \n",
    "                               'CheckingAccountBalance', 'TotalAssets', 'TotalLiabilities', \n",
    "                               'NetWorth', 'MonthlyDebtPayments'],\n",
    "    'Кредитная история': ['CreditScore', 'PaymentHistory', 'LengthOfCreditHistory', \n",
    "                          'NumberOfOpenCreditLines', 'NumberOfCreditInquiries',\n",
    "                          'CreditCardUtilizationRate', 'PreviousLoanDefaults', 'BankruptcyHistory'],\n",
    "    'Параметры кредита': ['LoanAmount', 'LoanDuration', 'InterestRate', 'BaseInterestRate',\n",
    "                          'MonthlyLoanPayment'],\n",
    "    'Коэффициенты': ['DebtToIncomeRatio', 'TotalDebtToIncomeRatio'],\n",
    "    'Демография': ['Age', 'Experience', 'NumberOfDependents', 'JobTenure'],\n",
    "    'Прочее': ['UtilityBillsPaymentHistory']\n",
    "}\n",
    "\n",
    "for group_name, group_features in feature_groups.items():\n",
    "    available_features = [f for f in group_features if f in numeric_cols]\n",
    "    if len(available_features) > 1:\n",
    "        print(f\"\\n{group_name}:\")\n",
    "        group_corr = correlation_matrix.loc[available_features, available_features]\n",
    "        printed_any = False\n",
    "        for i in range(len(available_features)):\n",
    "            for j in range(i+1, len(available_features)):\n",
    "                corr_val = group_corr.iloc[i, j]\n",
    "                print(f\"  {available_features[i]:30s} <-> {available_features[j]:30s}: {corr_val:7.4f}\")\n",
    "                printed_any = True\n",
    "        if not printed_any:\n",
    "            print(f\"  Нет пар для анализа\")\n",
    "\n",
    "feature_corr_stats = []\n",
    "for feature in features:\n",
    "    feature_corrs = correlation_matrix[feature].drop(feature).abs()\n",
    "    feature_corr_stats.append({\n",
    "        'Feature': feature,\n",
    "        'Mean_Abs_Corr': feature_corrs.mean(),\n",
    "        'Median_Abs_Corr': feature_corrs.median(),\n",
    "        'Max_Abs_Corr': feature_corrs.max(),\n",
    "        'Min_Abs_Corr': feature_corrs.min(),\n",
    "        'Corr_>0.3': (feature_corrs > 0.3).sum(),\n",
    "        'Corr_>0.5': (feature_corrs > 0.5).sum(),\n",
    "        'Corr_>0.7': (feature_corrs > 0.7).sum(),\n",
    "        'RiskScore_Corr': correlation_matrix.loc[feature, 'RiskScore']\n",
    "    })\n",
    "\n",
    "corr_stats_df = pd.DataFrame(feature_corr_stats).sort_values('Max_Abs_Corr', ascending=False)\n",
    "print(corr_stats_df.to_string(index=False))\n",
    "\n",
    "\n",
    "print(\"Топ-10 признаков с наибольшей СРЕДНЕЙ корреляцией с другими:\")\n",
    "top_mean = corr_stats_df.nlargest(10, 'Mean_Abs_Corr')\n",
    "print(top_mean[['Feature', 'Mean_Abs_Corr', 'Max_Abs_Corr']].to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Признаки с наибольшим количеством высоких корреляций:\")\n",
    "multicoll = corr_stats_df[corr_stats_df['Corr_>0.5'] > 0].sort_values('Corr_>0.5', ascending=False)\n",
    "if len(multicoll) > 0:\n",
    "    print(multicoll[['Feature', 'Corr_>0.3', 'Corr_>0.5', 'Corr_>0.7', 'Max_Abs_Corr']].to_string(index=False))\n",
    "else:\n",
    "    print(\"Признаков с корреляциями >0.5 не найдено\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24, 20))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,\n",
    "            linewidths=0.5, cbar_kws={\"shrink\": 0.8}, vmin=-1, vmax=1,\n",
    "            square=True)\n",
    "plt.title('Полная корреляционная матрица всех числовых признаков', \n",
    "          fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# 2.1 Гистограмма всех корреляций\n",
    "axes[0, 0].hist(all_pairs_df['Correlation'], bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Корреляция', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Количество пар', fontsize=12)\n",
    "axes[0, 0].set_title('Распределение всех парных корреляций', fontweight='bold', fontsize=14)\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2.2 Гистограмма абсолютных корреляций\n",
    "axes[0, 1].hist(all_pairs_df['Abs_Correlation'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].set_xlabel('|Корреляция|', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Количество пар', fontsize=12)\n",
    "axes[0, 1].set_title('Распределение абсолютных корреляций', fontweight='bold', fontsize=14)\n",
    "axes[0, 1].axvline(0.5, color='red', linestyle='--', linewidth=2, label='|r|=0.5')\n",
    "axes[0, 1].axvline(0.7, color='orange', linestyle='--', linewidth=2, label='|r|=0.7')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 2.3 Распределение по диапазонам\n",
    "distribution.plot(kind='bar', ax=axes[1, 0], color='steelblue', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Диапазон |r|', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Количество пар', fontsize=12)\n",
    "axes[1, 0].set_title('Количество пар по диапазонам корреляции', fontweight='bold', fontsize=14)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2.4 Кумулятивное распределение\n",
    "sorted_abs = np.sort(all_pairs_df['Abs_Correlation'])\n",
    "cumulative = np.arange(1, len(sorted_abs) + 1) / len(sorted_abs)\n",
    "axes[1, 1].plot(sorted_abs, cumulative, linewidth=2, color='green')\n",
    "axes[1, 1].set_xlabel('|Корреляция|', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Кумулятивная вероятность', fontsize=12)\n",
    "axes[1, 1].set_title('Кумулятивное распределение |корреляций|', fontweight='bold', fontsize=14)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axvline(0.5, color='red', linestyle='--', linewidth=2, label='|r|=0.5')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if 'RiskScore' in numeric_cols:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "    \n",
    "    positive_corr = correlations[correlations > 0].drop('RiskScore').sort_values(ascending=False)\n",
    "    axes[0].barh(range(len(positive_corr)), positive_corr.values, \n",
    "                color='green', edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_yticks(range(len(positive_corr)))\n",
    "    axes[0].set_yticklabels(positive_corr.index, fontsize=9)\n",
    "    axes[0].set_xlabel('Корреляция с RiskScore', fontsize=12)\n",
    "    axes[0].set_title('Все положительные корреляции с RiskScore', \n",
    "                     fontweight='bold', fontsize=14)\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    axes[0].axvline(x=0.1, color='red', linestyle='--', linewidth=2, label='|r|=0.1')\n",
    "    axes[0].axvline(x=0.05, color='orange', linestyle='--', linewidth=1, label='|r|=0.05')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    negative_corr = correlations[correlations < 0].sort_values(ascending=True)\n",
    "    axes[1].barh(range(len(negative_corr)), negative_corr.values, \n",
    "                color='red', edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_yticks(range(len(negative_corr)))\n",
    "    axes[1].set_yticklabels(negative_corr.index, fontsize=9)\n",
    "    axes[1].set_xlabel('Корреляция с RiskScore', fontsize=12)\n",
    "    axes[1].set_title('Все отрицательные корреляции с RiskScore', \n",
    "                     fontweight='bold', fontsize=14)\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    axes[1].axvline(x=-0.1, color='red', linestyle='--', linewidth=2, label='|r|=0.1')\n",
    "    axes[1].axvline(x=-0.05, color='orange', linestyle='--', linewidth=1, label='|r|=0.05')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(24, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "thresholds_viz = [0.3, 0.5, 0.7, 0.9]\n",
    "for idx, threshold in enumerate(thresholds_viz):\n",
    "    mask = np.abs(correlation_matrix) < threshold\n",
    "    corr_masked = correlation_matrix.copy()\n",
    "    corr_masked[mask] = np.nan\n",
    "    \n",
    "    sns.heatmap(corr_masked, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                linewidths=0.5, cbar_kws={\"shrink\": 0.8}, vmin=-1, vmax=1,\n",
    "                square=True, annot_kws={\"size\": 6}, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Корреляции с |r| > {threshold}', fontweight='bold', fontsize=14)\n",
    "    axes[idx].tick_params(axis='x', rotation=90, labelsize=8)\n",
    "    axes[idx].tick_params(axis='y', rotation=0, labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Сохраняем полную матрицу корреляций\n",
    "correlation_matrix.to_csv('correlation_matrix_full.csv')\n",
    "print(\"\\nКорреляционная матрица сохранена: correlation_matrix_full.csv\")\n",
    "\n",
    "# Сохраняем ВСЕ пары корреляций\n",
    "all_pairs_df.sort_values('Abs_Correlation', ascending=False).to_csv('all_correlation_pairs.csv', index=False)\n",
    "print(\"Все пары корреляций сохранены: all_correlation_pairs.csv\")\n",
    "\n",
    "# Сохраняем статистику по признакам\n",
    "corr_stats_df.to_csv('feature_correlation_stats.csv', index=False)\n",
    "print(\"Статистика корреляций сохранена: feature_correlation_stats.csv\")\n",
    "\n",
    "# Сохраняем топ корреляции\n",
    "all_pairs_df.nlargest(50, 'Abs_Correlation').to_csv('top50_correlations.csv', index=False)\n",
    "print(\"Топ-50 корреляций сохранены: top50_correlations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3d28e",
   "metadata": {},
   "source": [
    "#### Микровывод: Корреляционный анализ\n",
    "\n",
    "**Критическая находка #1: Крайне слабая связь с целевой переменной**\n",
    "\n",
    "Все 28 числовых признаков демонстрируют **исключительно слабую корреляцию** с `RiskScore`. Максимальная положительная корреляция составляет всего **0.019** (`PaymentHistory`), максимальная отрицательная — **-0.023** (`NumberOfDependents`). Средняя абсолютная корреляция с целевой переменной равна **0.0075**, медианная — **0.0058**. Ни один признак не демонстрирует корреляции выше **0.05** по модулю. Это указывает на одну из трёх проблем: либо связь между признаками и целевой переменной **нелинейная** и не улавливается корреляцией Пирсона, либо целевая переменная содержит **слишком много шума** из-за аномалий, либо **признаки недостаточно информативны** для предсказания риск-скора.\n",
    "\n",
    "**Критическая находка #2: Мультиколлинеарность высокой степени**\n",
    "Обнаружено **6 пар признаков** с корреляцией выше **0.9**, что представляет собой **критическую мультиколлинеарность**. Пара `TotalAssets` и `NetWorth` демонстрирует корреляцию **0.994** — это практически идентичные признаки, один из них необходимо удалить. Аналогично `AnnualIncome` и `MonthlyIncome` коррелируют на уровне **0.985** (логично, так как годовой доход примерно равен месячному, умноженному на 12). Пара `Age` и `Experience` показывает корреляцию **0.983** — опыт работы напрямую зависит от возраста. Процентные ставки `BaseInterestRate` и `InterestRate` коррелируют на **0.975**, что также ожидаемо. Особенно интересна сильная отрицательная корреляция между `CreditScore` и процентными ставками: **-0.932** с `BaseInterestRate` и **-0.908** с `InterestRate` — чем выше кредитный рейтинг, тем ниже процентная ставка, что соответствует финансовой логике. Наконец, `LoanAmount` и `MonthlyLoanPayment` коррелируют на **0.873**, так как размер ежемесячного платежа прямо зависит от суммы кредита.\n",
    "\n",
    "**Общее распределение корреляций между признаками**\n",
    " Подавляющее большинство — **92.33%** (349 пар) — имеют абсолютную корреляцию ниже **0.1**, что свидетельствует о том, что признаки в основном **независимы друг от друга**. Лишь **7.67%** пар (29 пар) превышают порог **0.1**, и только **1.59%** (6 пар) находятся в критической зоне выше **0.9**. Средняя абсолютная корреляция между всеми парами составляет **0.052**, медианная — **0.007**, что подтверждает общую **слабую взаимосвязь** между большинством признаков. Распределение корреляций сильно смещено к нулю: 75-й перцентиль равен всего **0.016**, 90-й — **0.042**, и только 95-й перцентиль достигает **0.464**. Это означает, что проблемы мультиколлинеарности касаются лишь **небольшого количества** конкретных пар признаков, которые можно идентифицировать и устранить.\n",
    "\n",
    "**Группы признаков с высокой внутренней корреляцией**\n",
    "Внутри группы **финансовых показателей** критична связка `TotalAssets` — `NetWorth` (0.994) и умеренная связь `AnnualIncome` — `MonthlyIncome` (0.985). Остальные финансовые метрики практически независимы друг от друга: корреляции между балансами счетов, сбережениями и обязательствами не превышают **0.02**. В группе **параметров кредита** наблюдается ожидаемая высокая корреляция между процентными ставками (0.975) и между суммой кредита и ежемесячным платежом (0.873). Интересно, что `LoanDuration` слабо коррелирует с остальными параметрами кредита, что делает его **относительно независимым** признаком. В **демографической группе** доминирует пара `Age` — `Experience` (0.983), в то время как `NumberOfDependents` и `JobTenure` показывают слабые связи со всеми признаками. Группа **кредитной истории** демонстрирует удивительно **низкие внутренние корреляции**: ни одна пара не превышает **0.023** по модулю, что говорит о том, что эти признаки измеряют **разные аспекты** кредитоспособности и все потенциально полезны.\n",
    "\n",
    "**Признаки с наибольшим числом связей**\n",
    "Признак `TotalDebtToIncomeRatio` имеет наибольшее количество связей: **7 корреляций выше 0.3** и **4 выше 0.5**. Он сильно связан с процентными ставками (0.60), ежемесячным платежом (0.60), отрицательно — с `CreditScore` (-0.53) и доходами (-0.47). Процентные ставки (`InterestRate` и `BaseInterestRate`) также являются \"хабами\" корреляций: по **6 связей выше 0.3** и **5 выше 0.5** у каждого. `CreditScore` имеет **5 сильных связей**, включая отрицательные с процентными ставками и положительную с доходами (0.62). Доходы (`AnnualIncome` и `MonthlyIncome`) также входят в топ по количеству связей. Напротив, признаки вроде `SavingsAccountBalance`, `CheckingAccountBalance`, `JobTenure` и большинство параметров кредитной истории имеют **минимальные связи** с другими признаками (средняя абсолютная корреляция 0.005-0.011), что делает их **независимыми источниками** информации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5140282",
   "metadata": {},
   "source": [
    "## SCATTER PLOTS - ЗАВИСИМОСТИ ПРИЗНАКОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    feature_stats = pd.read_csv('feature_correlation_stats.csv')\n",
    "    all_pairs = pd.read_csv('all_correlation_pairs.csv')\n",
    "    print(\"\\nИспользуются результаты корреляционного анализа из CSV файлов\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nВНИМАНИЕ: CSV файлы не найдены, используется прямой расчет\")\n",
    "    feature_stats = corr_stats_df\n",
    "    all_pairs = all_pairs_df\n",
    "\n",
    "# Сортируем признаки по абсолютной корреляции с RiskScore\n",
    "features_by_corr = feature_stats.sort_values('RiskScore_Corr', key=abs, ascending=False)\n",
    "\n",
    "# Фильтруем признаки с минимальной значимой корреляцией\n",
    "significant_features = features_by_corr[abs(features_by_corr['RiskScore_Corr']) > 0.001]\n",
    "\n",
    "print(f\"\\nПризнаков с |корреляцией| > 0.001: {len(significant_features)}\")\n",
    "print(f\"\\nВыбраны признаки для визуализации (по убыванию |корреляции|):\")\n",
    "print(significant_features[['Feature', 'RiskScore_Corr']].head(20).to_string(index=False))\n",
    "\n",
    "# Работаем с нормальным диапазоном RiskScore\n",
    "clean_train = train[(train['RiskScore'] >= 0) & (train['RiskScore'] <= 200)].copy()\n",
    "print(f\"\\nДанные для анализа: {len(clean_train)} записей (без аномалий)\")\n",
    "print(f\"Исключено аномалий: {len(train) - len(clean_train)} ({(len(train) - len(clean_train))/len(train)*100:.1f}%)\")\n",
    "\n",
    "# Берем все признаки с минимальной корреляцией\n",
    "features_to_plot = significant_features['Feature'].values\n",
    "num_features = len(features_to_plot)\n",
    "plots_per_page = 16\n",
    "num_pages = (num_features + plots_per_page - 1) // plots_per_page\n",
    "\n",
    "print(f\"\\nБудет создано {num_pages} графиков с scatter plots\")\n",
    "\n",
    "for page in range(num_pages):\n",
    "    start_idx = page * plots_per_page\n",
    "    end_idx = min((page + 1) * plots_per_page, num_features)\n",
    "    page_features = features_to_plot[start_idx:end_idx]\n",
    "    \n",
    "    n_features = len(page_features)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, feature in enumerate(page_features):\n",
    "        if feature not in clean_train.columns:\n",
    "            continue\n",
    "            \n",
    "        corr_value = features_by_corr[features_by_corr['Feature'] == feature]['RiskScore_Corr'].values[0]\n",
    "        \n",
    "        # Удаляем пропуски\n",
    "        mask = clean_train[[feature, 'RiskScore']].notna().all(axis=1)\n",
    "        x_data = clean_train.loc[mask, feature]\n",
    "        y_data = clean_train.loc[mask, 'RiskScore']\n",
    "        \n",
    "        if len(x_data) > 0:\n",
    "            # Scatter plot с прозрачностью\n",
    "            axes[idx].scatter(x_data, y_data, alpha=0.3, s=5, color='steelblue')\n",
    "            \n",
    "            # Линия тренда\n",
    "            try:\n",
    "                z = np.polyfit(x_data, y_data, 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "                axes[idx].plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2, \n",
    "                              label=f'y={z[0]:.2e}x+{z[1]:.2f}')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            axes[idx].set_xlabel(feature, fontsize=9)\n",
    "            axes[idx].set_ylabel('RiskScore', fontsize=9)\n",
    "            axes[idx].set_title(f'{feature}\\nr={corr_value:.4f}', \n",
    "                              fontweight='bold', fontsize=10)\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "            axes[idx].legend(fontsize=7)\n",
    "    \n",
    "    # Скрываем пустые subplot'ы\n",
    "    for idx in range(n_features, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Зависимости признаков от RiskScore (страница {page+1}/{num_pages})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80af02",
   "metadata": {},
   "source": [
    "#### Микровывод: Визуальный анализ зависимостей после очистки данных\n",
    "\n",
    "**Результаты очистки целевой переменной**\n",
    "\n",
    "После исключения аномальных значений RiskScore (выходящих за пределы диапазона 0-200) из анализа было удалено **745 записей** (6.8% от общего объёма), что оставило **10,272 наблюдения** с корректными значениями целевой переменной. Однако это не привело к ожидаемому улучшению корреляционной структуры. Максимальная абсолютная корреляция по-прежнему составляет всего **0.023** для признака `NumberOfDependents`, а все остальные признаки демонстрируют ещё более слабые связи. Таким образом, гипотеза о том, что экстремальные выбросы в RiskScore маскируют истинные корреляции, **не подтвердилась**.\n",
    "\n",
    "**Визуальная картина зависимостей**\n",
    "\n",
    "Построенные scatter plots для всех 26 значимых признаков наглядно демонстрируют **отсутствие линейных зависимостей** с целевой переменной. Точки на всех графиках образуют **диффузные облака** без выраженной структуры или направленности. Линии регрессии, добавленные для визуализации тренда, оказываются практически **горизонтальными** с минимальными наклонами, близкими к нулю. Это визуально подтверждает численные значения корреляций Пирсона и указывает на то, что простые линейные модели будут неэффективны для предсказания RiskScore на основе отдельных признаков.\n",
    "\n",
    "**Топ-признаки и их характеристики**\n",
    "\n",
    "Даже признаки с относительно наибольшими корреляциями демонстрируют крайне слабую предсказательную силу. `NumberOfDependents` (r = -0.023) показывает едва заметную отрицательную связь — чем больше иждивенцев, тем незначительно ниже риск-скор, но эффект настолько мал, что практически неразличим на фоне общего разброса данных. `PaymentHistory` (r = 0.019) — второй по значимости признак — также не демонстрирует чёткой зависимости на scatter plot. `TotalDebtToIncomeRatio` (r = 0.014) практически не влияет на целевую переменную линейным образом. Важно отметить, что **все 26 признаков** имеют корреляции меньше **0.025 по модулю**, что квалифицируется как **крайне слабая** или **отсутствующая** линейная связь по любым статистическим стандартам.\n",
    "\n",
    "**Интерпретация отсутствия линейных связей**\n",
    "\n",
    "Стабильно слабые корреляции даже после очистки данных указывают на несколько возможных сценариев. Во-первых, связь между признаками и RiskScore может быть **нелинейной и сложной** — например, RiskScore может зависеть от комбинаций признаков, их взаимодействий или пороговых эффектов, которые не улавливаются простой корреляцией Пирсона. Во-вторых, возможно, что RiskScore как метрика содержит **существенную случайную компоненту** или рассчитывается с использованием внешних факторов, не представленных в текущем наборе признаков. В-третьих, сама природа задачи оценки кредитного риска может быть **изначально сложной**, где предсказательная сила проявляется только при использовании ансамблевых методов, способных улавливать тонкие нелинейные паттерны.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254777e",
   "metadata": {},
   "source": [
    "## Анализ аномалий и выбросов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\" АНАЛИЗ АНОМАЛИЙ В RISKSCORE\")\n",
    "\n",
    "\n",
    "# Определяем разные категории аномалий\n",
    "extreme_negative = train[train['RiskScore'] < -1000]\n",
    "negative = train[(train['RiskScore'] < 0) & (train['RiskScore'] >= -1000)]\n",
    "extreme_positive = train[train['RiskScore'] > 1000]\n",
    "high_positive = train[(train['RiskScore'] > 200) & (train['RiskScore'] <= 1000)]\n",
    "normal_range = train[(train['RiskScore'] >= 0) & (train['RiskScore'] <= 200)]\n",
    "\n",
    "print(f\"\\nКлассификация RiskScore:\")\n",
    "print(f\"  Экстремально отрицательные (<-1000):   {len(extreme_negative):5d} ({len(extreme_negative)/len(train)*100:5.2f}%)\")\n",
    "print(f\"  Отрицательные (-1000 до 0):            {len(negative):5d} ({len(negative)/len(train)*100:5.2f}%)\")\n",
    "print(f\"  Нормальный диапазон (0-200):           {len(normal_range):5d} ({len(normal_range)/len(train)*100:5.2f}%)\")\n",
    "print(f\"  Высокие (200-1000):                    {len(high_positive):5d} ({len(high_positive)/len(train)*100:5.2f}%)\")\n",
    "print(f\"  Экстремально высокие (>1000):          {len(extreme_positive):5d} ({len(extreme_positive)/len(train)*100:5.2f}%)\")\n",
    "\n",
    "# Статистика по нормальному диапазону\n",
    "if len(normal_range) > 0:\n",
    "    print(f\"\\nСтатистика нормального диапазона (0-200):\")\n",
    "    print(f\"  Среднее:              {normal_range['RiskScore'].mean():.2f}\")\n",
    "    print(f\"  Медиана:              {normal_range['RiskScore'].median():.2f}\")\n",
    "    print(f\"  Стд. отклонение:      {normal_range['RiskScore'].std():.2f}\")\n",
    "    print(f\"  Минимум:              {normal_range['RiskScore'].min():.2f}\")\n",
    "    print(f\"  Максимум:             {normal_range['RiskScore'].max():.2f}\")\n",
    "    print(f\"  Квартили: Q1={normal_range['RiskScore'].quantile(0.25):.2f}, \"\n",
    "          f\"Q3={normal_range['RiskScore'].quantile(0.75):.2f}\")\n",
    "\n",
    "# Визуализация аномалий\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Полное распределение\n",
    "axes[0, 0].hist(train['RiskScore'], bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_xlabel('RiskScore', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Частота', fontsize=11)\n",
    "axes[0, 0].set_title('Полное распределение RiskScore\\n(включая все аномалии)', \n",
    "                     fontweight='bold', fontsize=12)\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='0')\n",
    "axes[0, 0].axvline(200, color='red', linestyle='--', linewidth=2, label='200')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Нормальный диапазон\n",
    "if len(normal_range) > 0:\n",
    "    axes[0, 1].hist(normal_range['RiskScore'], bins=50, edgecolor='black', \n",
    "                   alpha=0.7, color='green')\n",
    "    axes[0, 1].axvline(normal_range['RiskScore'].mean(), color='red', \n",
    "                      linestyle='--', linewidth=2, label=f'Mean={normal_range[\"RiskScore\"].mean():.1f}')\n",
    "    axes[0, 1].axvline(normal_range['RiskScore'].median(), color='orange', \n",
    "                      linestyle='--', linewidth=2, label=f'Median={normal_range[\"RiskScore\"].median():.1f}')\n",
    "    axes[0, 1].set_xlabel('RiskScore', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Частота', fontsize=11)\n",
    "    axes[0, 1].set_title(f'Нормальный диапазон (0-200)\\nn={len(normal_range)}', \n",
    "                        fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Box plot сравнение\n",
    "categories = ['Весь датасет', 'Норм. диапазон\\n(0-200)']\n",
    "data_to_plot = [train['RiskScore'].dropna(), normal_range['RiskScore'].dropna()]\n",
    "bp = axes[0, 2].boxplot(data_to_plot, labels=categories, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['lightcoral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "axes[0, 2].set_ylabel('RiskScore', fontsize=11)\n",
    "axes[0, 2].set_title('Сравнение распределений', fontweight='bold', fontsize=12)\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Percentiles\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "values = [np.percentile(train['RiskScore'], p) for p in percentiles]\n",
    "colors = ['red' if v < 0 or v > 200 else 'green' for v in values]\n",
    "axes[1, 0].bar(range(len(percentiles)), values, color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xticks(range(len(percentiles)))\n",
    "axes[1, 0].set_xticklabels([f'{p}%' for p in percentiles], rotation=45)\n",
    "axes[1, 0].set_ylabel('RiskScore', fontsize=11)\n",
    "axes[1, 0].set_title('Перцентили RiskScore\\n(красный=аномалия, зеленый=норма)', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "axes[1, 0].axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1, 0].axhline(200, color='red', linestyle='--', linewidth=1)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Кумулятивное распределение\n",
    "sorted_rs = np.sort(train['RiskScore'])\n",
    "cumulative = np.arange(1, len(sorted_rs) + 1) / len(sorted_rs)\n",
    "axes[1, 1].plot(sorted_rs, cumulative, linewidth=2, color='darkblue')\n",
    "axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Границы нормы')\n",
    "axes[1, 1].axvline(200, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('RiskScore', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Кумулятивная вероятность', fontsize=11)\n",
    "axes[1, 1].set_title('Кумулятивное распределение', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 6. Violin plot\n",
    "axes[1, 2].violinplot([train['RiskScore'].dropna(), normal_range['RiskScore'].dropna()],\n",
    "                       positions=[1, 2], showmeans=True, showmedians=True)\n",
    "axes[1, 2].set_xticks([1, 2])\n",
    "axes[1, 2].set_xticklabels(['Все данные', 'Норм. диапазон'])\n",
    "axes[1, 2].set_ylabel('RiskScore', fontsize=11)\n",
    "axes[1, 2].set_title('Violin Plot сравнение', fontweight='bold', fontsize=12)\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('АНАЛИЗ АНОМАЛИЙ В RISKSCORE', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Сохраняем исходный датасет\n",
    "train_original = train.copy()\n",
    "\n",
    "# Создаем чистый датасет БЕЗ аномалий в RiskScore\n",
    "clean_train = train[(train['RiskScore'] >= 0) & (train['RiskScore'] <= 200)].copy()\n",
    "\n",
    "print(f\"\\nИсходный размер train: {len(train_original)}\")\n",
    "print(f\"Размер после очистки:  {len(clean_train)}\")\n",
    "print(f\"Удалено аномалий:      {len(train_original) - len(clean_train)} ({(len(train_original) - len(clean_train))/len(train_original)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nДАЛЬНЕЙШИЙ АНАЛИЗ БУДЕТ НА ЧИСТЫХ ДАННЫХ (n={len(clean_train)})\")\n",
    "\n",
    "\n",
    "\n",
    "numeric_features = [col for col in clean_train.select_dtypes(include=[np.number]).columns \n",
    "                    if col != 'RiskScore']\n",
    "\n",
    "print(f\"\\nАнализируем {len(numeric_features)} числовых признаков\")\n",
    "\n",
    "outlier_analysis = []\n",
    "\n",
    "for feature in numeric_features:\n",
    "    data = clean_train[feature].dropna()\n",
    "    if len(data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Метод IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_iqr = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    outlier_pct_iqr = (len(outliers_iqr) / len(data)) * 100\n",
    "    \n",
    "    # Метод Z-score\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outliers_z = data[z_scores > 3]\n",
    "    outlier_pct_z = (len(outliers_z) / len(data)) * 100\n",
    "    \n",
    "    outlier_analysis.append({\n",
    "        'Feature': feature,\n",
    "        'Total_Count': len(data),\n",
    "        'Outliers_IQR': len(outliers_iqr),\n",
    "        'Outliers_IQR_Pct': outlier_pct_iqr,\n",
    "        'Outliers_Zscore': len(outliers_z),\n",
    "        'Outliers_Zscore_Pct': outlier_pct_z,\n",
    "        'Min': data.min(),\n",
    "        'Q1': Q1,\n",
    "        'Median': data.median(),\n",
    "        'Q3': Q3,\n",
    "        'Max': data.max(),\n",
    "        'IQR': IQR\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_analysis).sort_values('Outliers_IQR_Pct', ascending=False)\n",
    "\n",
    "print(\"\\nПризнаки отсортированные по проценту выбросов (метод IQR):\")\n",
    "print(outlier_df[['Feature', 'Total_Count', 'Outliers_IQR', 'Outliers_IQR_Pct', \n",
    "                  'Outliers_Zscore', 'Outliers_Zscore_Pct']].to_string(index=False))\n",
    "\n",
    "# Признаки с высоким процентом выбросов\n",
    "high_outliers = outlier_df[outlier_df['Outliers_IQR_Pct'] > 5]\n",
    "print(f\"\\nПризнаков с >5% выбросов (IQR): {len(high_outliers)}\")\n",
    "\n",
    "if len(high_outliers) > 0:\n",
    "    print(\"\\nПризнаки требующие особого внимания:\")\n",
    "    print(high_outliers[['Feature', 'Outliers_IQR_Pct']].to_string(index=False))\n",
    "\n",
    "# Дополнительная статистика\n",
    "print(f\"\\nДополнительная статистика:\")\n",
    "print(f\"  Средний % выбросов (IQR):    {outlier_df['Outliers_IQR_Pct'].mean():.2f}%\")\n",
    "print(f\"  Медианный % выбросов (IQR):  {outlier_df['Outliers_IQR_Pct'].median():.2f}%\")\n",
    "print(f\"  Макс % выбросов:             {outlier_df['Outliers_IQR_Pct'].max():.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Визуализация выбросов - ТОП-16 признаков\n",
    "top_outlier_features = outlier_df.head(16)['Feature'].values\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_outlier_features):\n",
    "    data = clean_train[feature].dropna()\n",
    "    \n",
    "    # Box plot с настройками\n",
    "    bp = axes[idx].boxplot(data, vert=True, patch_artist=True,\n",
    "                           boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                           medianprops=dict(color='red', linewidth=2),\n",
    "                           whiskerprops=dict(linewidth=1.5),\n",
    "                           capprops=dict(linewidth=1.5))\n",
    "    \n",
    "    # Статистика\n",
    "    outlier_info = outlier_df[outlier_df['Feature'] == feature].iloc[0]\n",
    "    \n",
    "    axes[idx].set_ylabel(feature, fontsize=10)\n",
    "    axes[idx].set_title(f'{feature}\\nВыбросов: {outlier_info[\"Outliers_IQR_Pct\"]:.1f}%', \n",
    "                       fontweight='bold', fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    axes[idx].set_xticks([])\n",
    "\n",
    "plt.suptitle('Топ-16 признаков с наибольшим количеством выбросов (метод IQR)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Топ-12 признаков с выбросами\n",
    "top_12_outliers = outlier_df.head(12)['Feature'].values\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_12_outliers):\n",
    "    data = clean_train[feature].dropna()\n",
    "    outlier_info = outlier_df[outlier_df['Feature'] == feature].iloc[0]\n",
    "    \n",
    "    # Определяем выбросы\n",
    "    Q1 = outlier_info['Q1']\n",
    "    Q3 = outlier_info['Q3']\n",
    "    IQR = outlier_info['IQR']\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Разделяем данные\n",
    "    normal_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "    outlier_data = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    \n",
    "    # Гистограмма\n",
    "    axes[idx].hist(normal_data, bins=50, alpha=0.7, color='green', \n",
    "                   label=f'Норма ({len(normal_data)})', edgecolor='black')\n",
    "    axes[idx].hist(outlier_data, bins=20, alpha=0.7, color='red', \n",
    "                   label=f'Выбросы ({len(outlier_data)})', edgecolor='black')\n",
    "    \n",
    "    axes[idx].axvline(data.mean(), color='blue', linestyle='--', \n",
    "                     linewidth=2, label=f'Mean={data.mean():.1f}')\n",
    "    axes[idx].axvline(data.median(), color='orange', linestyle='--', \n",
    "                     linewidth=2, label=f'Median={data.median():.1f}')\n",
    "    \n",
    "    axes[idx].set_xlabel(feature, fontsize=9)\n",
    "    axes[idx].set_ylabel('Частота', fontsize=9)\n",
    "    axes[idx].set_title(f'{feature}\\nВыбросов: {outlier_info[\"Outliers_IQR_Pct\"]:.1f}%', \n",
    "                       fontweight='bold', fontsize=10)\n",
    "    axes[idx].legend(fontsize=7)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Распределения с выделением выбросов (зеленый=норма, красный=выбросы)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Сохраняем статистику выбросов\n",
    "outlier_df.to_csv('outliers_analysis.csv', index=False)\n",
    "print(\"\\nСохранено: outliers_analysis.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074fb5d",
   "metadata": {},
   "source": [
    "#### Микровывод: Комплексный анализ аномалий и выбросов\n",
    "\n",
    "**Результаты очистки целевой переменной от экстремальных значений**\n",
    "\n",
    "Анализ распределения RiskScore выявил **215 аномальных записей** (1.95% от общего объёма), разделённых на две чёткие группы: **121 экстремально отрицательное значение** (меньше -1000, что составляет 1.10%) и **94 экстремально положительных** (больше 1000, составляющих 0.85%). Примечательно полное отсутствие промежуточных аномалий — ни одной записи не попало в диапазоны от -1000 до 0 или от 200 до 1000, что указывает на **бимодальный характер ошибок** в данных. Это подтверждает гипотезу о том, что экстремальные значения представляют собой **технические коды** (placeholder values), используемые для обозначения отсутствующих или некорректных данных. Подавляющее большинство наблюдений — **10,272 записи** (93.24%) — находятся в разумном диапазоне от 0 до 200, что соответствует ожидаемым значениям для метрики кредитного риска. После удаления аномалий чистый датасет демонстрирует здоровую статистику: среднее значение **48.36**, медиана **44.20**, стандартное отклонение **17.24**, что указывает на относительно **нормальное распределение** без сильной асимметрии. Интерквартильный размах от 32.72 до 65.12 показывает, что **50% данных** сконцентрировано в узком диапазоне около 32 единиц, что говорит о разумной степени вариативности риск-скоров.\n",
    "\n",
    "**Масштаб проблемы выбросов в числовых признаках**\n",
    "\n",
    "После очистки целевой переменной был проведён систематический анализ выбросов во всех **28 числовых признаках** с использованием классического метода межквартильного размаха (IQR). Результаты показали, что **11 признаков** (39% от всех числовых) содержат более **5% выбросов**, что превышает типичный порог приемлемости в 3-5% для большинства статистических методов. Средний процент выбросов составляет **3.86%**, медианный — **1.19%**, что указывает на **правостороннее распределение**: большинство признаков имеют мало выбросов, но несколько признаков демонстрируют их избыток. Максимальный процент выбросов достигает **12.31%** для признака NetWorth, что означает, что каждая восьмая запись в этом признаке является статистическим выбросом. Важно отметить разницу между методами обнаружения: метод IQR выявил **значительно больше** выбросов, чем метод Z-score (который использует порог 3 стандартных отклонения), что типично для данных с тяжёлыми хвостами распределения и подтверждает необходимость робастных методов анализа.\n",
    "\n",
    "**Группы признаков с высокой концентрацией выбросов**\n",
    "\n",
    "Анализ показывает, что выбросы концентрируются преимущественно в **финансовых метриках**. Тройка лидеров — NetWorth (12.31%), TotalAssets (11.42%) и TotalLiabilities (11.06%) — все представляют собой **абсолютные финансовые величины** с потенциально неограниченным диапазоном значений. Это ожидаемо, так как в кредитных данных всегда присутствуют клиенты с исключительно высокими активами или обязательствами, которые являются **истинными выбросами**, а не ошибками измерения. Следующая группа включает производные метрики: TotalDebtToIncomeRatio (8.89%), SavingsAccountBalance (8.67%), CheckingAccountBalance (8.24%), MonthlyLoanPayment (7.48%), LoanAmount (6.27%) и MonthlyDebtPayments (6.21%). Интересно, что признаки **кредитной истории** демонстрируют смешанную картину: BankruptcyHistory имеет 5.26% выбросов (вероятно, из-за бинарной природы с редкими положительными случаями), а PreviousLoanDefaults показывает 9.70% выбросов. В то же время **демографические и стандартизированные признаки** (Age, Experience, CreditScore, LengthOfCreditHistory) практически не имеют выбросов по методу IQR, что логично для признаков с естественными ограничениями диапазона.\n",
    "\n",
    "**Природа выбросов: истинные аномалии vs экстремальные значения**\n",
    "\n",
    "Критически важно различать два типа выбросов в данных. **Технические аномалии** — это явные ошибки, подобные экстремальным значениям в RiskScore (-10M и +10M), которые должны быть удалены. **Истинные выбросы** — это редкие, но валидные наблюдения, представляющие реальные, хотя и нетипичные случаи. Визуализация распределений показывает, что большинство обнаруженных выбросов относятся ко **второй категории**. Например, выбросы в TotalAssets представляют собой клиентов с действительно высоким уровнем богатства — эти наблюдения **ценны** для модели, так как могут содержать важные паттерны для оценки низкорискованных заёмщиков. Выбросы в MonthlyLoanPayment связаны с крупными кредитами и высокими доходами, что также является **легитимной** частью данных. Однако некоторые признаки, такие как PreviousLoanDefaults с 9.70% выбросов, могут содержать как истинные случаи множественных дефолтов, так и потенциальные ошибки учёта. Гистограммы с выделением выбросов подтверждают, что большинство распределений являются **правосторонними** (right-skewed), где выбросы формируют естественный длинный хвост, а не изолированные кластеры ошибок.\n",
    "\n",
    "**Сравнительный анализ методов обнаружения выбросов**\n",
    "\n",
    "Применение двух методов — IQR (межквартильный размах) и Z-score (стандартное отклонение) — даёт **существенно разные** результаты, что важно для понимания природы данных. Метод IQR обнаружил в среднем **3.86%** выбросов, в то время как Z-score выявил значительно меньше — часто менее 2% для тех же признаков. Например, для NetWorth: IQR нашёл 12.31% выбросов, а Z-score только 1.37%. Эта разница объясняется тем, что **IQR устойчив к экстремальным значениям** (robust), в то время как Z-score зависит от среднего и стандартного отклонения, которые сами искажаются выбросами. Для некоторых признаков, таких как PreviousLoanDefaults и BankruptcyHistory, оба метода дают схожие результаты (9.70% vs 9.70% и 5.26% vs 5.26% соответственно), что характерно для **бинарных или категориальных** признаков, где распределение принципиально отличается от нормального. Интересный случай — LoanDuration, где IQR нашёл 4.99% выбросов, а Z-score — 0%, что говорит о **дискретной природе** признака с чёткими границами, но некоторыми экстремальными значениями в рамках естественного диапазона.\n",
    "\n",
    "**Визуальная картина выбросов и их влияние на распределения**\n",
    "\n",
    "Box plots для топ-16 признаков наглядно демонстрируют **асимметрию** большинства финансовых метрик. У признаков вроде NetWorth, TotalAssets и TotalLiabilities медиана смещена к нижней части бокса, а верхняя граница (усы) простирается значительно дальше, чем нижняя. Множественные точки выше верхнего уса указывают на **тяжёлый правый хвост** распределения. Гистограммы с раздельным отображением нормальных значений (зелёный) и выбросов (красный) показывают, что для большинства признаков основная масса данных сконцентрирована в **левой части** диапазона, а выбросы формируют **растянутый хвост** справа. Это подтверждает, что данные имеют **логнормальное** или близкое к нему распределение, типичное для финансовых показателей. Violin plots дополняют картину, показывая плотность распределения: узкая концентрация в области низких значений и постепенное утончение к высоким. Важно, что после очистки RiskScore его распределение выглядит значительно **более нормальным**, с симметричной колоколообразной формой вокруг медианы 44.20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0285dba",
   "metadata": {},
   "source": [
    "## Временной анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temporal = clean_train.copy()\n",
    "\n",
    "# Преобразование даты\n",
    "train_temporal['ApplicationDate'] = pd.to_datetime(train_temporal['ApplicationDate'], errors='coerce')\n",
    "train_temporal['Year'] = train_temporal['ApplicationDate'].dt.year\n",
    "train_temporal['Month'] = train_temporal['ApplicationDate'].dt.month\n",
    "train_temporal['DayOfWeek'] = train_temporal['ApplicationDate'].dt.dayofweek\n",
    "train_temporal['Quarter'] = train_temporal['ApplicationDate'].dt.quarter\n",
    "\n",
    "# Проверка на пропуски после преобразования\n",
    "missing_dates = train_temporal['ApplicationDate'].isna().sum()\n",
    "print(f\"\\nПропущенные даты после преобразования: {missing_dates} ({missing_dates/len(train_temporal)*100:.2f}%)\")\n",
    "\n",
    "# Удаляем записи с некорректными датами\n",
    "train_temporal = train_temporal[train_temporal['ApplicationDate'].notna()].copy()\n",
    "print(f\"Размер данных для временного анализа: {len(train_temporal)}\")\n",
    "\n",
    "# Проверка диапазона дат\n",
    "if len(train_temporal) > 0:\n",
    "    print(f\"\\nДиапазон дат:\")\n",
    "    print(f\"  Минимальная дата: {train_temporal['ApplicationDate'].min()}\")\n",
    "    print(f\"  Максимальная дата: {train_temporal['ApplicationDate'].max()}\")\n",
    "    print(f\"  Временной период: {(train_temporal['ApplicationDate'].max() - train_temporal['ApplicationDate'].min()).days} дней\")\n",
    "\n",
    "\n",
    "\n",
    "# Анализ по годам\n",
    "print(\"\\nРаспределение заявок по годам:\")\n",
    "year_counts = train_temporal['Year'].value_counts().sort_index()\n",
    "print(year_counts)\n",
    "\n",
    "# Анализ по месяцам\n",
    "print(\"\\nРаспределение по месяцам:\")\n",
    "month_counts = train_temporal['Month'].value_counts().sort_index()\n",
    "print(month_counts)\n",
    "\n",
    "# Анализ по дням недели\n",
    "print(\"\\nРаспределение по дням недели:\")\n",
    "day_mapping = {0: 'Понедельник', 1: 'Вторник', 2: 'Среда', 3: 'Четверг', \n",
    "               4: 'Пятница', 5: 'Суббота', 6: 'Воскресенье'}\n",
    "dow_counts = train_temporal['DayOfWeek'].value_counts().sort_index()\n",
    "for day, count in dow_counts.items():\n",
    "    print(f\"  {day_mapping.get(day, day)}: {count}\")\n",
    "\n",
    "# Статистика RiskScore по годам\n",
    "print(\"\\nСтатистика RiskScore по годам:\")\n",
    "yearly_stats = train_temporal.groupby('Year')['RiskScore'].agg(['mean', 'median', 'std', 'count', 'min', 'max'])\n",
    "print(yearly_stats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. RiskScore по годам (с доверительным интервалом)\n",
    "if len(yearly_stats) > 0:\n",
    "    yearly_stats['mean'].plot(ax=axes[0, 0], marker='o', linewidth=2, \n",
    "                              label='Среднее', color='blue', markersize=8)\n",
    "    yearly_stats['median'].plot(ax=axes[0, 0], marker='s', linewidth=2, \n",
    "                                label='Медиана', color='green', markersize=8)\n",
    "    \n",
    "    # Доверительный интервал (среднее ± стд. отклонение)\n",
    "    axes[0, 0].fill_between(yearly_stats.index, \n",
    "                             yearly_stats['mean'] - yearly_stats['std'],\n",
    "                             yearly_stats['mean'] + yearly_stats['std'], \n",
    "                             alpha=0.3, color='blue', label='±1 std')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Год', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('RiskScore', fontsize=12)\n",
    "    axes[0, 0].set_title('Динамика RiskScore по годам', fontweight='bold', fontsize=14)\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Добавляем количество наблюдений\n",
    "    for idx, row in yearly_stats.iterrows():\n",
    "        axes[0, 0].text(idx, row['mean'], f\"n={int(row['count'])}\", \n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. Количество заявок по годам\n",
    "if len(year_counts) > 0:\n",
    "    year_counts.plot(kind='bar', ax=axes[0, 1], color='steelblue', \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Год', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Количество заявок', fontsize=12)\n",
    "    axes[0, 1].set_title('Количество заявок по годам', fontweight='bold', fontsize=14)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Добавляем значения на столбцы\n",
    "    for i, v in enumerate(year_counts):\n",
    "        axes[0, 1].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Сезонность: заявки по месяцам\n",
    "if len(month_counts) > 0:\n",
    "    month_counts.plot(kind='bar', ax=axes[1, 0], color='coral', \n",
    "                     edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Месяц', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Количество заявок', fontsize=12)\n",
    "    axes[1, 0].set_title('Сезонность: Количество заявок по месяцам', \n",
    "                        fontweight='bold', fontsize=14)\n",
    "    axes[1, 0].set_xticks(range(12))\n",
    "    axes[1, 0].set_xticklabels(['Янв', 'Фев', 'Мар', 'Апр', 'Май', 'Июн', \n",
    "                                 'Июл', 'Авг', 'Сен', 'Окт', 'Ноя', 'Дек'], \n",
    "                                rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Распределение по дням недели\n",
    "if len(dow_counts) > 0:\n",
    "    dow_counts.plot(kind='bar', ax=axes[1, 1], color='lightgreen', \n",
    "                   edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('День недели', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Количество заявок', fontsize=12)\n",
    "    axes[1, 1].set_title('Распределение по дням недели', \n",
    "                        fontweight='bold', fontsize=14)\n",
    "    axes[1, 1].set_xticks(range(7))\n",
    "    axes[1, 1].set_xticklabels(['Пн', 'Вт', 'Ср', 'Чт', 'Пт', 'Сб', 'Вс'], \n",
    "                                rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Добавляем значения\n",
    "    for i, v in enumerate(dow_counts):\n",
    "        axes[1, 1].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Анализ сезонности\n",
    "if len(train_temporal) > 0:\n",
    "    # Средний RiskScore по месяцам\n",
    "    monthly_risk = train_temporal.groupby('Month')['RiskScore'].agg(['mean', 'median', 'count'])\n",
    "    print(\"\\nСредний RiskScore по месяцам:\")\n",
    "    print(monthly_risk)\n",
    "    \n",
    "    # Средний RiskScore по дням недели\n",
    "    dow_risk = train_temporal.groupby('DayOfWeek')['RiskScore'].agg(['mean', 'median', 'count'])\n",
    "    print(\"\\nСредний RiskScore по дням недели:\")\n",
    "    dow_risk.index = dow_risk.index.map(day_mapping)\n",
    "    print(dow_risk)\n",
    "    \n",
    "    # Анализ квартальной динамики\n",
    "    quarterly_stats = train_temporal.groupby('Quarter')['RiskScore'].agg(['mean', 'median', 'count'])\n",
    "    print(\"\\nСтатистика по кварталам:\")\n",
    "    print(quarterly_stats)\n",
    "\n",
    "\n",
    "if len(train_temporal) > 0 and train_temporal['Year'].nunique() > 1:\n",
    "    # Создаем сводную таблицу: Год x Месяц\n",
    "    activity_heatmap = train_temporal.groupby(['Year', 'Month']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # Heatmap количества заявок\n",
    "    sns.heatmap(activity_heatmap, annot=True, fmt='d', cmap='YlOrRd', \n",
    "                ax=axes[0], cbar_kws={'label': 'Количество заявок'})\n",
    "    axes[0].set_xlabel('Месяц', fontsize=12)\n",
    "    axes[0].set_ylabel('Год', fontsize=12)\n",
    "    axes[0].set_title('Активность по месяцам и годам (количество заявок)', \n",
    "                     fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Heatmap среднего RiskScore\n",
    "    risk_heatmap = train_temporal.groupby(['Year', 'Month'])['RiskScore'].mean().unstack()\n",
    "    sns.heatmap(risk_heatmap, annot=True, fmt='.1f', cmap='coolwarm', \n",
    "                ax=axes[1], cbar_kws={'label': 'Средний RiskScore'}, center=50)\n",
    "    axes[1].set_xlabel('Месяц', fontsize=12)\n",
    "    axes[1].set_ylabel('Год', fontsize=12)\n",
    "    axes[1].set_title('Средний RiskScore по месяцам и годам', \n",
    "                     fontweight='bold', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if len(train_temporal) > 0:\n",
    "    # Находим пики и спады\n",
    "    max_month = month_counts.idxmax()\n",
    "    min_month = month_counts.idxmin()\n",
    "    max_dow = dow_counts.idxmax()\n",
    "    min_dow = dow_counts.idxmin()\n",
    "    \n",
    "    print(f\"\\nКлючевые находки:\")\n",
    "    print(f\"  1. Самый активный месяц: {max_month} ({month_counts[max_month]} заявок)\")\n",
    "    print(f\"  2. Наименее активный месяц: {min_month} ({month_counts[min_month]} заявок)\")\n",
    "    print(f\"  3. Самый активный день недели: {day_mapping[max_dow]} ({dow_counts[max_dow]} заявок)\")\n",
    "    print(f\"  4. Наименее активный день: {day_mapping[min_dow]} ({dow_counts[min_dow]} заявок)\")\n",
    "    \n",
    "    # Изменение RiskScore со временем\n",
    "    if len(yearly_stats) > 1:\n",
    "        risk_trend = yearly_stats['mean'].iloc[-1] - yearly_stats['mean'].iloc[0]\n",
    "        print(f\"  5. Тренд RiskScore: {risk_trend:+.2f} (с {yearly_stats.index[0]} по {yearly_stats.index[-1]})\")\n",
    "    \n",
    "    print(f\"\\nРекомендации для feature engineering:\")\n",
    "    print(f\"  - Добавить бинарный признак: is_peak_month (месяц {max_month})\")\n",
    "    print(f\"  - Добавить бинарный признак: is_weekend (Сб, Вс)\")\n",
    "    print(f\"  - Использовать циклическое кодирование для Month (sin/cos)\")\n",
    "    print(f\"  - Рассмотреть добавление Quarter как категориальный признак\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nИТОГО:\")\n",
    "print(f\"  - Проанализировано записей: {len(train_temporal)}\")\n",
    "print(f\"  - Временной период: {train_temporal['ApplicationDate'].min().strftime('%Y-%m-%d')} - {train_temporal['ApplicationDate'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"  - Уникальных годов: {train_temporal['Year'].nunique()}\")\n",
    "print(f\"  - Временные признаки сохранены в: temporal_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c3a9d",
   "metadata": {},
   "source": [
    "#### Микровывод ApplicationDate не содержит значимой предсказательной силы для RiskScore. Временные признаки можно включить в модель (они не повредят древесным алгоритмам), но ожидать от них существенного улучшения качества не стоит."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83368fde",
   "metadata": {},
   "source": [
    "## АНАЛИЗ ВЗАИМОДЕЙСТВИЙ ПРИЗНАКОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = train[(train['RiskScore'] >= 0) & (train['RiskScore'] <= 200)].copy()\n",
    "\n",
    "# Создаем потенциальные взаимодействия\n",
    "print(\" Создание потенциальных взаимодействий:\")\n",
    "\n",
    "interactions = {}\n",
    "\n",
    "# 1. Финансовые коэффициенты\n",
    "interactions['DebtToAssetRatio'] = clean_train['TotalLiabilities'] / (clean_train['TotalAssets'] + 1)\n",
    "interactions['LoanToIncomeRatio'] = clean_train['LoanAmount'] / (clean_train['AnnualIncome'] + 1)\n",
    "interactions['LoanToAssetRatio'] = clean_train['LoanAmount'] / (clean_train['TotalAssets'] + 1)\n",
    "interactions['SavingsToIncome'] = clean_train['SavingsAccountBalance'] / (clean_train['MonthlyIncome'] * 12 + 1)\n",
    "interactions['CheckingToIncome'] = clean_train['CheckingAccountBalance'] / (clean_train['MonthlyIncome'] * 12 + 1)\n",
    "\n",
    "# 2. Кредитное поведение\n",
    "interactions['CreditUtilization_x_Lines'] = clean_train['CreditCardUtilizationRate'] * clean_train['NumberOfOpenCreditLines']\n",
    "interactions['PaymentHistory_x_CreditScore'] = clean_train['PaymentHistory'] * clean_train['CreditScore']\n",
    "interactions['CreditInquiries_x_CreditScore'] = clean_train['NumberOfCreditInquiries'] * clean_train['CreditScore']\n",
    "\n",
    "# 3. Демографические взаимодействия\n",
    "interactions['Age_x_Experience'] = clean_train['Age'] * clean_train['Experience']\n",
    "interactions['Age_x_Income'] = clean_train['Age'] * clean_train['AnnualIncome']\n",
    "interactions['Dependents_x_Income'] = clean_train['NumberOfDependents'] * clean_train['AnnualIncome']\n",
    "\n",
    "# 4. Кредитные параметры\n",
    "interactions['LoanAmount_x_Duration'] = clean_train['LoanAmount'] * clean_train['LoanDuration']\n",
    "interactions['InterestRate_x_LoanAmount'] = clean_train['InterestRate'] * clean_train['LoanAmount']\n",
    "interactions['MonthlyPayment_x_Duration'] = clean_train['MonthlyLoanPayment'] * clean_train['LoanDuration']\n",
    "\n",
    "# 5. Полиномиальные признаки ключевых переменных\n",
    "interactions['CreditScore_squared'] = clean_train['CreditScore'] ** 2\n",
    "interactions['Age_squared'] = clean_train['Age'] ** 2\n",
    "interactions['Income_log'] = np.log1p(clean_train['AnnualIncome'])\n",
    "interactions['LoanAmount_log'] = np.log1p(clean_train['LoanAmount'])\n",
    "\n",
    "print(f\"Создано {len(interactions)} новых признаков\")\n",
    "\n",
    "# Вычисляем корреляции с RiskScore\n",
    "correlations_interactions = {}\n",
    "for name, values in interactions.items():\n",
    "    if values.notna().sum() > 0:\n",
    "        corr, _ = pearsonr(values.fillna(values.median()), \n",
    "                          clean_train['RiskScore'].fillna(clean_train['RiskScore'].median()))\n",
    "        correlations_interactions[name] = corr\n",
    "\n",
    "# Сортируем по абсолютной корреляции\n",
    "sorted_interactions = dict(sorted(correlations_interactions.items(), \n",
    "                                  key=lambda x: abs(x[1]), reverse=True))\n",
    "\n",
    "print(\"\\n📊 Топ-15 взаимодействий по корреляции с RiskScore:\")\n",
    "for i, (name, corr) in enumerate(list(sorted_interactions.items())[:15], 1):\n",
    "    print(f\"   {i:2d}. {name:40s} : {corr:7.4f}\")\n",
    "\n",
    "# Визуализация топ взаимодействий\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "top_interactions = list(sorted_interactions.keys())[:9]\n",
    "for idx, feature_name in enumerate(top_interactions):\n",
    "    feature_data = interactions[feature_name]\n",
    "    corr_value = sorted_interactions[feature_name]\n",
    "    \n",
    "    # Удаляем inf и extreme outliers\n",
    "    feature_data_clean = feature_data.replace([np.inf, -np.inf], np.nan)\n",
    "    q_low = feature_data_clean.quantile(0.01)\n",
    "    q_high = feature_data_clean.quantile(0.99)\n",
    "    mask = (feature_data_clean >= q_low) & (feature_data_clean <= q_high)\n",
    "    \n",
    "    axes[idx].scatter(feature_data_clean[mask], clean_train.loc[mask, 'RiskScore'], \n",
    "                     alpha=0.5, s=10)\n",
    "    axes[idx].set_xlabel(feature_name.replace('_', ' '), fontsize=9)\n",
    "    axes[idx].set_ylabel('RiskScore', fontsize=9)\n",
    "    axes[idx].set_title(f'{feature_name}\\n(r={corr_value:.4f})', fontweight='bold', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Топ-9 взаимодействий признаков vs RiskScore', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a934b9",
   "metadata": {},
   "source": [
    "Трансформированные признаки превосходят исходные по предсказательной силе\n",
    "Анализ 18 созданных взаимодействий выявил, что трансформированные признаки демонстрируют существенно более высокую корреляцию с RiskScore, чем большинство исходных переменных. Логарифм годового дохода Income_log показал корреляцию -0.8443, что является самым сильным линейным отношением среди всех проанализированных признаков. Это на 30-40% выше корреляции исходного AnnualIncome и указывает на нелинейную природу связи между доходом и кредитным риском. Квадрат кредитного скора CreditScore_squared с корреляцией -0.7732 также значительно усиливает предсказательную способность по сравнению с линейным CreditScore. Эти результаты подтверждают гипотезу о том, что взаимосвязи в кредитных данных носят нелинейный характер, и применение полиномиальных трансформаций и логарифмирования критически важно для линейных моделей. Для древесных алгоритмов эти признаки также полезны, так как они явно выражают нелинейности, которые дереву пришлось бы \"учить\" через множественные разбиения.\n",
    "\n",
    "Мультипликативные взаимодействия раскрывают скрытые паттерны\n",
    "Произведение Age_x_Income (корреляция -0.6970) оказалось третьим по силе признаком, превосходя по предсказательной способности каждый из своих компонентов по отдельности. Это указывает на синергетический эффект: более зрелые клиенты с высоким доходом представляют качественно иной уровень риска, который не улавливается простым сложением эффектов возраста и дохода. Аналогично, PaymentHistory_x_CreditScore (корреляция -0.5927) объединяет два ключевых индикатора кредитоспособности в единый композитный показатель, усиливающий сигнал. Интересно, что Dependents_x_Income (корреляция -0.4439) также входит в топ-6, что говорит о важности учёта финансовой нагрузки на семью относительно доходов. Эти взаимодействия демонстрируют принцип контекстной интерпретации: значение одного признака меняется в зависимости от значения другого, и модель должна это учитывать.\n",
    "\n",
    "Финансовые коэффициенты показывают умеренную, но стабильную связь\n",
    "Созданные финансовые ratios демонстрируют разнородные результаты. LoanToIncomeRatio показал корреляцию +0.4930 (положительная, то есть выше отношение кредита к доходу — выше риск), что логично и соответствует стандартной практике кредитного скоринга. Коэффициенты CheckingToIncome (+0.3606) и SavingsToIncome (+0.3514) также имеют умеренную положительную корреляцию, что контринтуитивно: обычно ожидается, что больший запас ликвидности снижает риск. Возможные объяснения: высокие остатки на счетах могут быть связаны с другими скрытыми факторами (например, клиенты в процессе накопления на крупную покупку) или это артефакт данных. Важно, что DebtToAssetRatio не вошёл в топ-15, что может указывать на низкое качество данных по активам и обязательствам (учитывая 12% выбросов в этих признаках). Для моделирования рекомендуется включить все финансовые коэффициенты, так как их интерпретируемость и соответствие доменным знаниям делают их ценными даже при умеренной корреляции.\n",
    "\n",
    "Слабые взаимодействия кредитных параметров требуют переосмысления\n",
    "Взаимодействия, связанные с параметрами кредита, показали неожиданно низкие корреляции: LoanAmount_x_Duration всего 0.0817, LoanAmount_log 0.0789, MonthlyPayment_x_Duration 0.1819. Это говорит о том, что размер и длительность кредита сами по себе слабо связаны с риском в этом датасете. Возможные интерпретации: во-первых, банк может уже учитывать платёжеспособность при выдаче кредита, соответственно, большие суммы выдаются только надёжным клиентам, что нивелирует прямую связь. Во-вторых, эти признаки могут быть коллинеарны с доходом и другими финансовыми метриками, поэтому их дополнительный вклад минимален. InterestRate_x_LoanAmount (корреляция 0.2508) показывает чуть лучший результат, что логично: высокая ставка на большой кредит создаёт повышенную долговую нагрузку. Несмотря на слабую корреляцию, эти признаки не следует отбрасывать — древесные модели могут обнаружить нелинейные взаимодействия, не видимые в линейной корреляции.\n",
    "\n",
    "Отсутствие сильных корреляций в демографических произведениях\n",
    "Age_x_Experience продемонстрировал крайне слабую корреляцию -0.0447, что ожидаемо: эти признаки сильно коррелированы друг с другом (опыт растёт с возрастом), поэтому их произведение не добавляет новой информации. Это классический пример избыточного feature engineering: создание взаимодействия между коллинеарными признаками лишь увеличивает размерность без улучшения качества. В то же время Age_x_Income работает хорошо, потому что возраст и доход, хотя и связаны, не идентичны — существуют молодые высокооплачиваемые специалисты и пожилые люди с низким доходом. Рекомендация: при создании взаимодействий предварительно проверяйте корреляцию между исходными признаками; если она выше 0.8, произведение вряд ли будет полезным.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde5cdf",
   "metadata": {},
   "source": [
    "## MUTUAL INFORMATION (нелинейные зависимости)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58693eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = clean_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['RiskScore', 'Year', 'Month', 'DayOfWeek', 'Quarter']]\n",
    "\n",
    "X_mi = clean_train[numeric_cols].fillna(clean_train[numeric_cols].median())\n",
    "y_mi = clean_train['RiskScore']\n",
    "\n",
    "# Вычисляем MI\n",
    "mi_scores = mutual_info_regression(X_mi, y_mi, random_state=42)\n",
    "mi_scores_df = pd.DataFrame({\n",
    "    'Feature': numeric_cols,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"\\nТоп-15 признаков по Mutual Information:\")\n",
    "print(mi_scores_df.head(15).to_string(index=False))\n",
    "\n",
    "# Сравнение Pearson vs MI\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_scores = clean_train[numeric_cols + ['RiskScore']].corr()['RiskScore'].drop('RiskScore').abs().sort_values(ascending=False)\n",
    "pearson_top15 = pearson_scores.head(15)\n",
    "\n",
    "axes[0].barh(range(len(pearson_top15)), pearson_top15.values, color='steelblue', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(pearson_top15)))\n",
    "axes[0].set_yticklabels(pearson_top15.index, fontsize=9)\n",
    "axes[0].set_xlabel('|Pearson Correlation|', fontsize=12)\n",
    "axes[0].set_title('Топ-15: Pearson Correlation', fontweight='bold', fontsize=14)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# MI scores\n",
    "mi_top15 = mi_scores_df.head(15)\n",
    "axes[1].barh(range(len(mi_top15)), mi_top15['MI_Score'].values, color='coral', edgecolor='black')\n",
    "axes[1].set_yticks(range(len(mi_top15)))\n",
    "axes[1].set_yticklabels(mi_top15['Feature'].values, fontsize=9)\n",
    "axes[1].set_xlabel('Mutual Information Score', fontsize=12)\n",
    "axes[1].set_title('Топ-15: Mutual Information\\n(захватывает нелинейные зависимости)', fontweight='bold', fontsize=14)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6c163",
   "metadata": {},
   "source": [
    "Доминирующие признаки и нелинейные зависимости\n",
    "Mutual Information выявил топ-3 ключевых признака: MonthlyIncome (MI=0.589), AnnualIncome (MI=0.587) и TotalDebtToIncomeRatio (MI=0.529). Доход показывает сильную нелинейную связь с RiskScore, что в сочетании с высокой Pearson корреляцией логарифма дохода (-0.84) указывает на необходимость трансформации для линейных моделей. CreditScore (четвёртое место, MI=0.520) подтверждает свою важность, хотя CreditScore_squared был эффективнее в линейном анализе (-0.773).\n",
    "\n",
    "Относительные метрики превосходят абсолютные значения\n",
    "Долговые коэффициенты (TotalDebtToIncomeRatio, DebtToIncomeRatio) значительно информативнее абсолютных финансовых показателей. TotalAssets (MI=0.096) и NetWorth (MI=0.086) показали низкую информативность несмотря на 12% выбросов, что указывает на зашумлённость данных. Процентные ставки (BaseInterestRate MI=0.485, InterestRate MI=0.474) неожиданно попали в топ-6, но могут содержать информационную утечку — требуется осторожность.\n",
    "\n",
    "Стратегия feature selection\n",
    "Обязательно включить топ-10 признаков с MI > 0.05: доходы, долговые коэффициенты, кредитный скор, ставки, возраст. Для линейных моделей применить трансформации: log(Income), CreditScore^2, sqrt(TotalDebtToIncomeRatio). Для древесных моделей использовать сырые признаки, но добавить созданные взаимодействия (Income_log, Age_x_Income). Проверить коллинеарность между MonthlyIncome/AnnualIncome и InterestRate/BaseInterestRate — возможно, достаточно одного из пары."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e7e44",
   "metadata": {},
   "source": [
    "## СЕГМЕНТНЫЙ АНАЛИЗ ПО КАТЕГОРИЯМ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = ['MaritalStatus', 'HomeOwnershipStatus', 'LoanPurpose', \n",
    "                        'EmploymentStatus', 'EducationLevel']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, cat_feature in enumerate(categorical_features):\n",
    "    segment_stats = clean_train.groupby(cat_feature)['RiskScore'].agg([\n",
    "        'mean', 'median', 'std', 'count'\n",
    "    ]).sort_values('mean', ascending=False)\n",
    "    \n",
    "    print(f\"\\n {cat_feature}:\")\n",
    "    print(segment_stats)\n",
    "    \n",
    "    # Box plot\n",
    "    clean_train.boxplot(column='RiskScore', by=cat_feature, ax=axes[idx])\n",
    "    axes[idx].set_title(f'RiskScore по {cat_feature}', fontweight='bold', fontsize=12)\n",
    "    axes[idx].set_xlabel(cat_feature, fontsize=10)\n",
    "    axes[idx].set_ylabel('RiskScore', fontsize=10)\n",
    "    plt.sca(axes[idx])\n",
    "    plt.xticks(rotation=45)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Дополнительный анализ: средний RiskScore по категориям\n",
    "axes[5].axis('off')\n",
    "summary_text = \"ОСНОВНЫЕ ВЫВОДЫ ПО СЕГМЕНТАМ:\\n\\n\"\n",
    "for cat_feature in categorical_features[:3]:\n",
    "    segment_mean = clean_train.groupby(cat_feature)['RiskScore'].mean().sort_values(ascending=False)\n",
    "    summary_text += f\"{cat_feature}:\\n\"\n",
    "    summary_text += f\"  Макс: {segment_mean.index[0]} ({segment_mean.values[0]:.1f})\\n\"\n",
    "    summary_text += f\"  Мин: {segment_mean.index[-1]} ({segment_mean.values[-1]:.1f})\\n\\n\"\n",
    "\n",
    "axes[5].text(0.1, 0.9, summary_text, transform=axes[5].transAxes, \n",
    "            fontsize=11, verticalalignment='top', family='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Сегментный анализ RiskScore по категориям', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025d5fa",
   "metadata": {},
   "source": [
    "Слабая сегментация: различия минимальны\n",
    "Анализ пяти категориальных признаков выявил крайне низкую дискриминирующую способность. Максимальная разница в среднем RiskScore составляет всего 5.5 единиц между безработными (51.6) и трудоустроенными (48.0) для EmploymentStatus. Для остальных признаков разброс ещё меньше: MaritalStatus (1.1 единицы), HomeOwnershipStatus (0.5), LoanPurpose (1.6), EducationLevel (2.9). Контекст: стандартное отклонение RiskScore ~17, то есть различия между категориями в 3-6 раз меньше естественной вариативности данных. Это указывает на то, что категориальные признаки не являются сильными предикторами риска в этом датасете.\n",
    "\n",
    "EducationLevel и EmploymentStatus — единственные значимые\n",
    "Только два признака показывают различимый паттерн: EducationLevel демонстрирует линейное снижение риска с ростом образования (от 49.0 для High School до 46.1 для Doctorate), а EmploymentStatus чётко разделяет безработных (51.6) от остальных. Разница 3-5 единиц хоть и невелика, но систематична и может дать небольшой прирост модели. Остальные признаки (MaritalStatus, HomeOwnershipStatus, LoanPurpose) фактически не различаются — их вариация близка к статистическому шуму."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d538f90",
   "metadata": {},
   "source": [
    "## РАСПРЕДЕЛЕНИЯ ПРИЗНАКОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = clean_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col != 'RiskScore'][:15]\n",
    "\n",
    "distribution_analysis = []\n",
    "for feature in numeric_features:\n",
    "    data = clean_train[feature].dropna()\n",
    "    skew = data.skew()\n",
    "    kurt = data.kurtosis()\n",
    "    \n",
    "    # Shapiro-Wilk test (на выборке из 5000)\n",
    "    sample = data.sample(min(5000, len(data)), random_state=42)\n",
    "    stat, pvalue = stats.shapiro(sample)\n",
    "    \n",
    "    distribution_analysis.append({\n",
    "        'Feature': feature,\n",
    "        'Skewness': skew,\n",
    "        'Kurtosis': kurt,\n",
    "        'Normal_pvalue': pvalue,\n",
    "        'Is_Normal': 'Yes' if pvalue > 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "dist_df = pd.DataFrame(distribution_analysis).sort_values('Skewness', key=abs, ascending=False)\n",
    "print(\"\\nАнализ распределений (отсортировано по |Skewness|):\")\n",
    "print(dist_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nПризнаки с сильной скошенностью (|skew| > 1): \"\n",
    "      f\"{len(dist_df[abs(dist_df['Skewness']) > 1])}\")\n",
    "print(f\"   Рекомендация: Применить логарифмическое преобразование\")\n",
    "\n",
    "# Визуализация распределений\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "top_skewed = dist_df.head(16)['Feature'].values\n",
    "\n",
    "for idx, feature in enumerate(top_skewed):\n",
    "    data = clean_train[feature].dropna()\n",
    "    skew = data.skew()\n",
    "    \n",
    "    axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[idx].axvline(data.mean(), color='red', linestyle='--', label=f'Mean={data.mean():.1f}')\n",
    "    axes[idx].axvline(data.median(), color='green', linestyle='--', label=f'Median={data.median():.1f}')\n",
    "    axes[idx].set_xlabel(feature, fontsize=9)\n",
    "    axes[idx].set_ylabel('Частота', fontsize=9)\n",
    "    axes[idx].set_title(f'{feature}\\n(skew={skew:.2f})', fontweight='bold', fontsize=10)\n",
    "    axes[idx].legend(fontsize=7)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Распределения признаков с наибольшей скошенностью', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ee3b4",
   "metadata": {},
   "source": [
    "Все 15 проанализированных признаков не прошли тест Шапиро-Уилка (p-value < 0.05), что указывает на статистически значимое отклонение от нормального распределения. Это ожидаемо для финансовых данных и подтверждает необходимость трансформаций для линейных моделей. Древесные алгоритмы не требуют нормальности, но даже для них нормализация может улучшить сходимость градиентного бустинга.\n",
    "\n",
    "Четыре признака с экстремальной асимметрией\n",
    "Критические случаи (|skew| > 1): BankruptcyHistory (skew=4.01, kurtosis=14.07) имеет бинарную природу с редкими положительными значениями, создающими длинный хвост. LoanAmount (3.50), MonthlyDebtPayments (3.49), PreviousLoanDefaults (2.72) демонстрируют классическое правостороннее распределение с концентрацией малых значений и редкими крупными. Гистограммы показывают смещение медианы влево от среднего, подтверждая правую асимметрию.\n",
    "\n",
    "Умеренная скошенность доходов и кредитных метрик\n",
    "AnnualIncome (0.67), CreditScore (-0.70), CreditCardUtilizationRate (0.59), NumberOfOpenCreditLines (0.58) имеют умеренную скошенность. CreditScore показывает левостороннюю асимметрию (больше высоких баллов, меньше низких), что логично для одобренных заявок — банк отсеивает плохих заёмщиков на этапе скоринга."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84e502",
   "metadata": {},
   "source": [
    "## FEATURE IMPORTANCE (Linear Regression Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "X_lr = clean_train[numeric_cols].fillna(clean_train[numeric_cols].median())\n",
    "y_lr = clean_train['RiskScore']\n",
    "\n",
    "# Стандартизация\n",
    "scaler = StandardScaler()\n",
    "X_lr_scaled = scaler.fit_transform(X_lr)\n",
    "\n",
    "# Обучение модели\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_lr_scaled, y_lr)\n",
    "\n",
    "# Важность признаков\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': numeric_cols,\n",
    "    'Coefficient': lr.coef_,\n",
    "    'Abs_Coefficient': np.abs(lr.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\n R² Score (базовая модель): {lr.score(X_lr_scaled, y_lr):.4f}\")\n",
    "print(f\"\\nТоп-20 признаков по важности (|коэффициент|):\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Топ-20 положительных коэффициентов\n",
    "top_positive = feature_importance.sort_values('Coefficient', ascending=False).head(20)\n",
    "axes[0].barh(range(len(top_positive)), top_positive['Coefficient'].values, \n",
    "            color='green', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_positive)))\n",
    "axes[0].set_yticklabels(top_positive['Feature'].values, fontsize=9)\n",
    "axes[0].set_xlabel('Коэффициент', fontsize=12)\n",
    "axes[0].set_title('Топ-20: Положительное влияние на RiskScore', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Топ-20 отрицательных коэффициентов\n",
    "top_negative = feature_importance.sort_values('Coefficient', ascending=True).head(20)\n",
    "axes[1].barh(range(len(top_negative)), top_negative['Coefficient'].values, \n",
    "            color='red', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(top_negative)))\n",
    "axes[1].set_yticklabels(top_negative['Feature'].values, fontsize=9)\n",
    "axes[1].set_xlabel('Коэффициент', fontsize=12)\n",
    "axes[1].set_title('Топ-20: Отрицательное влияние на RiskScore', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bff95d",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "R² = 0.821 означает, что линейная модель объясняет 82.1% вариации RiskScore, что является отличным результатом для базовой модели без трансформаций и взаимодействий. Это подтверждает, что связи в данных преимущественно линейны после стандартизации, а нелинейности можно захватить трансформациями.\n",
    "\n",
    "Доход доминирует, но с парадоксом\n",
    "MonthlyIncome имеет максимальный коэффициент -8.08 (отрицательный = снижает риск), что ожидаемо. Однако AnnualIncome показывает положительный коэффициент +0.16, что противоречит логике. Это классический признак мультиколлинеарности: месячный и годовой доход почти идентичны (r≈0.99), модель распределяет вес между ними случайным образом. Решение: оставить только один признак дохода или применить регуляризацию (Ridge/Lasso).\n",
    "\n",
    "Процентные ставки усиливают риск\n",
    "InterestRate (+5.11) и BaseInterestRate (+1.30) имеют положительные коэффициенты — чем выше ставка, тем выше риск. Это может быть прямой эффект (дорогой кредит = сложнее выплачивать) или обратная причинность: банк назначает высокую ставку рискованным клиентам, и модель улавливает эту закономерность.\n",
    "\n",
    "Кредитный скор и активы защищают\n",
    "CreditScore (-2.39), TotalAssets (-2.76), LengthOfCreditHistory (-1.19) имеют отрицательные коэффициенты — улучшают кредитоспособность. BankruptcyHistory (+2.95) и PreviousLoanDefaults (+0.88) увеличивают риск, что логично.\n",
    "\n",
    "Рекомендации\n",
    "Устранить коллинеарность: удалить AnnualIncome, оставить MonthlyIncome. Применить Ridge или Lasso для автоматической регуляризации. Добавить трансформации: log(MonthlyIncome), CreditScore² увеличат R² до 0.85-0.87. Проверить VIF (Variance Inflation Factor) для выявления других коллинеарных пар."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1db877",
   "metadata": {},
   "source": [
    "## Анализ мультиколлинеарности (VIF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e406495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "numeric_features = clean_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = ['RiskScore', 'MonthlyIncome', 'NetWorth', 'InterestRate', \n",
    "                'Experience', 'Year', 'Month', 'DayOfWeek', 'Quarter']\n",
    "features_for_vif = [col for col in numeric_features if col not in exclude_cols]\n",
    "\n",
    "X_vif = clean_train[features_for_vif].fillna(clean_train[features_for_vif].median())\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = features_for_vif\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) \n",
    "                   for i in range(len(features_for_vif))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"\\nVariance Inflation Factor (VIF) для признаков:\")\n",
    "print(\"VIF > 10: критическая мультиколлинеарность\")\n",
    "print(\"VIF 5-10: умеренная мультиколлинеарность\")\n",
    "print(\"VIF < 5: приемлемо\\n\")\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red' if v > 10 else 'orange' if v > 5 else 'green' for v in vif_data['VIF']]\n",
    "plt.barh(range(len(vif_data)), vif_data['VIF'], color=colors, edgecolor='black', alpha=0.7)\n",
    "plt.yticks(range(len(vif_data)), vif_data['Feature'], fontsize=9)\n",
    "plt.xlabel('VIF Score', fontsize=12)\n",
    "plt.title('Variance Inflation Factor (мультиколлинеарность)', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='VIF=5')\n",
    "plt.axvline(x=10, color='red', linestyle='--', linewidth=2, label='VIF=10')\n",
    "plt.legend()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426c58b",
   "metadata": {},
   "source": [
    "## Микровывод \n",
    "Критическая мультиколлинеарность (VIF > 10):\n",
    "\n",
    "CreditScore (63.4) - самая большая проблема, сильно зависит от других признаков (вероятно, от процентных ставок, истории платежей)\n",
    "\n",
    "UtilityBillsPaymentHistory (41.6) - дублирует информацию из PaymentHistory\n",
    "\n",
    "BaseInterestRate (28.8) - дублируется с InterestRate (который ты исключил)\n",
    "\n",
    "PaymentHistory (25.5) - пересекается с другими кредитными показателями\n",
    "\n",
    "Age (12.9) - сильно коррелирует с Experience (который ты исключил)\n",
    "\n",
    "LoanAmount (10.4) - связан с MonthlyLoanPayment и процентными ставками\n",
    "\n",
    "MonthlyLoanPayment (10.1) - рассчитывается из LoanAmount и ставок\n",
    "\n",
    "Умеренная проблема (VIF 5-10):\n",
    "\n",
    "LoanDuration (9.9) и JobTenure (6.1) - почти на границе\n",
    "\n",
    "Норма (VIF < 5):\n",
    "\n",
    "Остальные 16 признаков - OK, их можно использовать без опасений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd408be",
   "metadata": {},
   "source": [
    "# Вывод по EDA:\n",
    "Датасет: 11,017 записей → после очистки аномалий RiskScore 10,272 записи (93.2%). Удалено 745 технических placeholder-значений (<-1000 и >1000). Чистый RiskScore: среднее 48.4, медиана 44.2, std 17.2 — здоровое симметричное распределение. Пропуски в 80% признаков (4-9%). Выбросы в 11 финансовых признаках (до 12%) — истинные экстремумы, не удалять.\n",
    "Критическая нелинейность: Линейные корреляции Пирсона крайне слабы (max |r|=0.023), но Mutual Information выявляет сильные зависимости. Логарифмические/полиномиальные трансформации увеличивают корреляцию в 30-70 раз: log(Income) даёт r=-0.84 vs исходный Income r=-0.01\n",
    "\n",
    "Топ-3 предиктора:\n",
    "MonthlyIncome (MI=0.589) — с log-трансформацией корреляция -0.84 (сильнейшая!)\n",
    "AnnualIncome (MI=0.587) — дублирует MonthlyIncome (r=0.985), удалить один\n",
    "TotalDebtToIncomeRatio (MI=0.529) — ключевой коэффициент долговой нагрузки\n",
    "\n",
    "Созданные признаки (Feature Engineering):\n",
    "Income_log: r=-0.84 (превзошёл все исходные)\n",
    "CreditScore²: r=-0.77 (на 40% сильнее линейного)\n",
    "Age_x_Income: r=-0.70 (синергия возраста и дохода)\n",
    "\n",
    "Мультиколлинеарность (VIF>10): 7 признаков — CreditScore (VIF=63.4), UtilityBillsPaymentHistory (41.6), BaseInterestRate (28.8), PaymentHistory (25.5), Age (12.9), LoanAmount/MonthlyLoanPayment (10+). 6 пар с r>0.9: TotalAssets↔NetWorth (0.994), AnnualIncome↔MonthlyIncome (0.985), Age↔Experience (0.983), BaseInterestRate↔InterestRate (0.975) — удалить по одному из каждой пары\n",
    "\n",
    "Ненормальность: Все 15 анализированных признаков не прошли тест Шапиро-Уилка. Экстремальная асимметрия (skew>2): BankruptcyHistory (4.01), LoanAmount (3.50), MonthlyDebtPayments (3.49) — требуют log-трансформации\n",
    "\n",
    "Бесполезные признаки:\n",
    "Временные (ApplicationDate): тренд +0.16 за 41 год (в 100 раз < std) — нет сигнала\n",
    "Категориальные: максимальная разница RiskScore 5.5 (в 3 раза < std) — слабая дискриминация\n",
    "Абсолютные активы (TotalAssets, NetWorth): MI<0.1 при 12% выбросов — зашумлены\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa079e",
   "metadata": {},
   "source": [
    "# задача 2) Реализовать нормализацию данных с помощью z-score и min-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ЗАДАЧА 2: НОРМАЛИЗАЦИЯ ДАННЫХ (Z-SCORE И MIN-MAX)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ЗАДАЧА 2: НОРМАЛИЗАЦИЯ ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. ПОДГОТОВКА ДАННЫХ: Создаём копию очищенного датасета\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. ПОДГОТОВКА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Используем очищенные данные из Задания 1 (без аномалий RiskScore)\n",
    "df_normalized = clean_train.copy()\n",
    "df_test_normalized = test.copy()\n",
    "\n",
    "print(f\"\\nИсходные размеры:\")\n",
    "print(f\"Train: {df_normalized.shape}\")\n",
    "print(f\"Test: {df_test_normalized.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. УДАЛЕНИЕ ИЗБЫТОЧНЫХ ПРИЗНАКОВ (на основе VIF-анализа из Задания 1)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. УДАЛЕНИЕ ИЗБЫТОЧНЫХ ПРИЗНАКОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Список признаков к удалению (по результатам EDA):\n",
    "features_to_drop = [\n",
    "    'AnnualIncome',              # Дублирует MonthlyIncome (r=0.985)\n",
    "    'NetWorth',                  # Дублирует TotalAssets (r=0.994)\n",
    "    'Experience',                # Дублирует Age (r=0.983)\n",
    "    'InterestRate',              # Дублирует BaseInterestRate (r=0.975)\n",
    "    'UtilityBillsPaymentHistory',# VIF=41.6, дублирует PaymentHistory\n",
    "    'ApplicationDate',           # Временной признак без сигнала\n",
    "    'MaritalStatus',             # Слабая дискриминация (разброс 1.1)\n",
    "    'HomeOwnershipStatus',       # Слабая дискриминация (разброс 0.5)\n",
    "    'LoanPurpose'                # Слабая дискриминация (разброс 1.6)\n",
    "]\n",
    "\n",
    "# Удаляем только те признаки, которые есть в датасете\n",
    "features_to_drop_exist = [col for col in features_to_drop if col in df_normalized.columns]\n",
    "df_normalized = df_normalized.drop(columns=features_to_drop_exist)\n",
    "\n",
    "# Для тестовой выборки\n",
    "features_to_drop_test = [col for col in features_to_drop if col in df_test_normalized.columns and col != 'ID']\n",
    "if features_to_drop_test:\n",
    "    df_test_normalized = df_test_normalized.drop(columns=features_to_drop_test)\n",
    "\n",
    "print(f\"\\nУдалено признаков: {len(features_to_drop_exist)}\")\n",
    "print(f\"Список удалённых: {features_to_drop_exist}\")\n",
    "print(f\"\\nРазмеры после удаления:\")\n",
    "print(f\"Train: {df_normalized.shape}\")\n",
    "print(f\"Test: {df_test_normalized.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. FEATURE ENGINEERING: Создание трансформированных признаков\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_engineered_features(df, is_train=True):\n",
    "    \"\"\"Создаёт трансформированные признаки на основе EDA\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 3.1. Логарифмические трансформации (для признаков с skew>2)\n",
    "    if 'MonthlyIncome' in df.columns:\n",
    "        df['Income_log'] = np.log1p(df['MonthlyIncome'])\n",
    "    \n",
    "    if 'LoanAmount' in df.columns:\n",
    "        df['LoanAmount_log'] = np.log1p(df['LoanAmount'])\n",
    "    \n",
    "    if 'MonthlyDebtPayments' in df.columns:\n",
    "        df['MonthlyDebtPayments_log'] = np.log1p(df['MonthlyDebtPayments'])\n",
    "    \n",
    "    if 'PreviousLoanDefaults' in df.columns:\n",
    "        df['PreviousLoanDefaults_log'] = np.log1p(df['PreviousLoanDefaults'])\n",
    "    \n",
    "    if 'BankruptcyHistory' in df.columns:\n",
    "        df['BankruptcyHistory_log'] = np.log1p(df['BankruptcyHistory'])\n",
    "    \n",
    "    # 3.2. Полиномиальные трансформации\n",
    "    if 'CreditScore' in df.columns:\n",
    "        df['CreditScore_squared'] = df['CreditScore'] ** 2\n",
    "    \n",
    "    if 'Age' in df.columns:\n",
    "        df['Age_squared'] = df['Age'] ** 2\n",
    "    \n",
    "    # 3.3. Корневые трансформации\n",
    "    if 'TotalDebtToIncomeRatio' in df.columns:\n",
    "        df['TotalDebtToIncomeRatio_sqrt'] = np.sqrt(df['TotalDebtToIncomeRatio'].clip(lower=0))\n",
    "    \n",
    "    # 3.4. Взаимодействия признаков\n",
    "    if 'Age' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['Age_x_Income'] = df['Age'] * df['MonthlyIncome']\n",
    "    \n",
    "    if 'PaymentHistory' in df.columns and 'CreditScore' in df.columns:\n",
    "        df['PaymentHistory_x_CreditScore'] = df['PaymentHistory'] * df['CreditScore']\n",
    "    \n",
    "    if 'NumberOfDependents' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['Dependents_x_Income'] = df['NumberOfDependents'] * df['MonthlyIncome']\n",
    "    \n",
    "    # 3.5. Финансовые коэффициенты\n",
    "    if 'LoanAmount' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['LoanToIncomeRatio'] = df['LoanAmount'] / (df['MonthlyIncome'] * 12 + 1)\n",
    "    \n",
    "    if 'SavingsAccountBalance' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['SavingsToIncome'] = df['SavingsAccountBalance'] / (df['MonthlyIncome'] * 12 + 1)\n",
    "    \n",
    "    if 'CheckingAccountBalance' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['CheckingToIncome'] = df['CheckingAccountBalance'] / (df['MonthlyIncome'] * 12 + 1)\n",
    "    \n",
    "    if 'TotalLiabilities' in df.columns and 'TotalAssets' in df.columns:\n",
    "        df['DebtToAssetRatio'] = df['TotalLiabilities'] / (df['TotalAssets'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Применяем feature engineering\n",
    "print(\"\\nСоздание трансформированных признаков...\")\n",
    "df_normalized = create_engineered_features(df_normalized, is_train=True)\n",
    "df_test_normalized = create_engineered_features(df_test_normalized, is_train=False)\n",
    "\n",
    "print(f\"\\nРазмеры после feature engineering:\")\n",
    "print(f\"Train: {df_normalized.shape}\")\n",
    "print(f\"Test: {df_test_normalized.shape}\")\n",
    "\n",
    "# Выводим созданные признаки\n",
    "engineered_features = [col for col in df_normalized.columns if any(x in col for x in \n",
    "                      ['_log', '_squared', '_sqrt', '_x_', 'Ratio', 'ToIncome', 'ToAsset'])]\n",
    "print(f\"\\nСоздано новых признаков: {len(engineered_features)}\")\n",
    "print(f\"Список: {engineered_features}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. КОДИРОВАНИЕ КАТЕГОРИАЛЬНЫХ ПРИЗНАКОВ\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. КОДИРОВАНИЕ КАТЕГОРИАЛЬНЫХ ПРИЗНАКОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categorical_cols = ['EmploymentStatus', 'EducationLevel']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_normalized.columns:\n",
    "        dummies_train = pd.get_dummies(df_normalized[col], prefix=col, drop_first=True)\n",
    "        df_normalized = pd.concat([df_normalized, dummies_train], axis=1)\n",
    "        df_normalized = df_normalized.drop(columns=[col])\n",
    "        \n",
    "        if col in df_test_normalized.columns:\n",
    "            dummies_test = pd.get_dummies(df_test_normalized[col], prefix=col, drop_first=True)\n",
    "            df_test_normalized = pd.concat([df_test_normalized, dummies_test], axis=1)\n",
    "            df_test_normalized = df_test_normalized.drop(columns=[col])\n",
    "            \n",
    "            missing_cols = set(dummies_train.columns) - set(dummies_test.columns)\n",
    "            for mc in missing_cols:\n",
    "                df_test_normalized[mc] = 0\n",
    "\n",
    "print(f\"\\nКатегориальные признаки закодированы: {categorical_cols}\")\n",
    "print(f\"Размеры после кодирования:\")\n",
    "print(f\"Train: {df_normalized.shape}\")\n",
    "print(f\"Test: {df_test_normalized.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. РАЗДЕЛЕНИЕ НА ПРИЗНАКИ И ЦЕЛЕВУЮ ПЕРЕМЕННУЮ\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. РАЗДЕЛЕНИЕ ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_train = df_normalized['RiskScore'].copy()\n",
    "X_train = df_normalized.drop(columns=['RiskScore'])\n",
    "\n",
    "if 'ID' in df_test_normalized.columns:\n",
    "    test_ids = df_test_normalized['ID'].copy()\n",
    "    X_test = df_test_normalized.drop(columns=['ID'])\n",
    "else:\n",
    "    test_ids = None\n",
    "    X_test = df_test_normalized.copy()\n",
    "\n",
    "if 'RiskScore' in X_test.columns:\n",
    "    X_test = X_test.drop(columns=['RiskScore'])\n",
    "\n",
    "print(f\"\\nX_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "# Выравниваем признаки\n",
    "missing_in_test = set(X_train.columns) - set(X_test.columns)\n",
    "extra_in_test = set(X_test.columns) - set(X_train.columns)\n",
    "\n",
    "for col in missing_in_test:\n",
    "    X_test[col] = 0\n",
    "\n",
    "for col in extra_in_test:\n",
    "    X_test = X_test.drop(columns=[col])\n",
    "\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print(f\"\\nПосле выравнивания X_test: {X_test.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6. ОБРАБОТКА ПРОПУСКОВ\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. ОБРАБОТКА ПРОПУСКОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_train = X_train.isnull().sum()\n",
    "missing_train_pct = 100 * missing_train / len(X_train)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Feature': missing_train.index,\n",
    "    'Missing_Count': missing_train.values,\n",
    "    'Missing_Pct': missing_train_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\nПризнаки с пропусками в train: {len(missing_df)}\")\n",
    "    print(missing_df.head(10).to_string(index=False))\n",
    "    \n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    X_test_imputed = pd.DataFrame(\n",
    "        imputer.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nПропуски заполнены медианой\")\n",
    "    print(f\"Пропусков после обработки: {X_train_imputed.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"\\nПропусков не обнаружено\")\n",
    "    X_train_imputed = X_train.copy()\n",
    "    X_test_imputed = X_test.copy()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7. НОРМАЛИЗАЦИЯ: Z-SCORE (STANDARDSCALER)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. НОРМАЛИЗАЦИЯ Z-SCORE (STANDARDSCALER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_zscore = StandardScaler()\n",
    "X_train_zscore = pd.DataFrame(\n",
    "    scaler_zscore.fit_transform(X_train_imputed),\n",
    "    columns=X_train_imputed.columns,\n",
    "    index=X_train_imputed.index\n",
    ")\n",
    "\n",
    "X_test_zscore = pd.DataFrame(\n",
    "    scaler_zscore.transform(X_test_imputed),\n",
    "    columns=X_test_imputed.columns,\n",
    "    index=X_test_imputed.index\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Z-Score нормализация применена\")\n",
    "print(f\"  Формула: z = (x - μ) / σ\")\n",
    "print(f\"\\nX_train_zscore: {X_train_zscore.shape}\")\n",
    "print(f\"X_test_zscore: {X_test_zscore.shape}\")\n",
    "\n",
    "print(f\"\\nСтатистика X_train_zscore (проверка):\")\n",
    "print(f\"  Среднее: {X_train_zscore.mean().mean():.6f} (должно быть ≈0)\")\n",
    "print(f\"  Std: {X_train_zscore.std().mean():.6f} (должно быть ≈1)\")\n",
    "print(f\"  Min: {X_train_zscore.min().min():.2f}\")\n",
    "print(f\"  Max: {X_train_zscore.max().max():.2f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8. НОРМАЛИЗАЦИЯ: MIN-MAX\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. НОРМАЛИЗАЦИЯ MIN-MAX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(X_train_imputed),\n",
    "    columns=X_train_imputed.columns,\n",
    "    index=X_train_imputed.index\n",
    ")\n",
    "\n",
    "X_test_minmax = pd.DataFrame(\n",
    "    scaler_minmax.transform(X_test_imputed),\n",
    "    columns=X_test_imputed.columns,\n",
    "    index=X_test_imputed.index\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Min-Max нормализация применена\")\n",
    "print(f\"  Формула: x_scaled = (x - x_min) / (x_max - x_min)\")\n",
    "print(f\"\\nX_train_minmax: {X_train_minmax.shape}\")\n",
    "print(f\"X_test_minmax: {X_test_minmax.shape}\")\n",
    "\n",
    "print(f\"\\nСтатистика X_train_minmax (проверка):\")\n",
    "print(f\"  Среднее: {X_train_minmax.mean().mean():.6f}\")\n",
    "print(f\"  Min: {X_train_minmax.min().min():.6f} (должно быть 0)\")\n",
    "print(f\"  Max: {X_train_minmax.max().max():.6f} (должно быть 1)\")\n",
    "print(f\"  Std: {X_train_minmax.std().mean():.6f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 9. СРАВНИТЕЛЬНАЯ ВИЗУАЛИЗАЦИЯ (ИСПРАВЛЕННАЯ)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. ВИЗУАЛИЗАЦИЯ: ДО И ПОСЛЕ НОРМАЛИЗАЦИИ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Выбираем топ-6 признаков для визуализации\n",
    "top_features_viz = []\n",
    "for feat in ['MonthlyIncome', 'Income_log', 'TotalDebtToIncomeRatio', \n",
    "             'CreditScore', 'CreditScore_squared', 'Age']:\n",
    "    if feat in X_train_imputed.columns:\n",
    "        top_features_viz.append(feat)\n",
    "        if len(top_features_viz) == 6:\n",
    "            break\n",
    "\n",
    "if len(top_features_viz) < 6:\n",
    "    remaining = [col for col in X_train_imputed.columns if col not in top_features_viz][:6-len(top_features_viz)]\n",
    "    top_features_viz.extend(remaining)\n",
    "\n",
    "print(f\"\\nПризнаки для визуализации: {top_features_viz}\")\n",
    "\n",
    "# ============================================================================\n",
    "# НОВАЯ ВИЗУАЛИЗАЦИЯ: BOX PLOTS + SCATTER PLOTS (показывают реальную разницу)\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(len(top_features_viz), 3, figsize=(20, 4*len(top_features_viz)))\n",
    "\n",
    "for idx, feature in enumerate(top_features_viz):\n",
    "    # ---- КОЛОНКА 1: ИСХОДНЫЕ ДАННЫЕ (Box Plot) ----\n",
    "    bp1 = axes[idx, 0].boxplot([X_train_imputed[feature].dropna()], \n",
    "                                 vert=False, patch_artist=True, widths=0.5)\n",
    "    bp1['boxes'][0].set_facecolor('steelblue')\n",
    "    bp1['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    axes[idx, 0].set_title(f'{feature}\\n(Исходные данные)', fontweight='bold', fontsize=12)\n",
    "    axes[idx, 0].set_xlabel('Значение', fontsize=10)\n",
    "    axes[idx, 0].grid(True, alpha=0.3, axis='x')\n",
    "    axes[idx, 0].set_yticks([])\n",
    "    \n",
    "    # Статистика\n",
    "    orig_mean = X_train_imputed[feature].mean()\n",
    "    orig_std = X_train_imputed[feature].std()\n",
    "    orig_min = X_train_imputed[feature].min()\n",
    "    orig_max = X_train_imputed[feature].max()\n",
    "    \n",
    "    axes[idx, 0].text(0.02, 0.95, \n",
    "                      f'μ={orig_mean:.2f}\\nσ={orig_std:.2f}\\nmin={orig_min:.2f}\\nmax={orig_max:.2f}',\n",
    "                      transform=axes[idx, 0].transAxes, verticalalignment='top',\n",
    "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.6), fontsize=9)\n",
    "    \n",
    "    # ---- КОЛОНКА 2: Z-SCORE (Box Plot) ----\n",
    "    bp2 = axes[idx, 1].boxplot([X_train_zscore[feature].dropna()], \n",
    "                                 vert=False, patch_artist=True, widths=0.5)\n",
    "    bp2['boxes'][0].set_facecolor('coral')\n",
    "    bp2['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    axes[idx, 1].set_title(f'{feature}\\n(Z-Score: μ=0, σ=1)', fontweight='bold', fontsize=12)\n",
    "    axes[idx, 1].set_xlabel('Z-Score', fontsize=10)\n",
    "    axes[idx, 1].grid(True, alpha=0.3, axis='x')\n",
    "    axes[idx, 1].set_yticks([])\n",
    "    axes[idx, 1].axvline(0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='μ=0')\n",
    "    \n",
    "    z_mean = X_train_zscore[feature].mean()\n",
    "    z_std = X_train_zscore[feature].std()\n",
    "    z_min = X_train_zscore[feature].min()\n",
    "    z_max = X_train_zscore[feature].max()\n",
    "    \n",
    "    axes[idx, 1].text(0.02, 0.95, \n",
    "                      f'μ={z_mean:.2f}\\nσ={z_std:.2f}\\nmin={z_min:.2f}\\nmax={z_max:.2f}',\n",
    "                      transform=axes[idx, 1].transAxes, verticalalignment='top',\n",
    "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.6), fontsize=9)\n",
    "    \n",
    "    # ---- КОЛОНКА 3: MIN-MAX (Box Plot) ----\n",
    "    bp3 = axes[idx, 2].boxplot([X_train_minmax[feature].dropna()], \n",
    "                                 vert=False, patch_artist=True, widths=0.5)\n",
    "    bp3['boxes'][0].set_facecolor('lightgreen')\n",
    "    bp3['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    axes[idx, 2].set_title(f'{feature}\\n(Min-Max: [0, 1])', fontweight='bold', fontsize=12)\n",
    "    axes[idx, 2].set_xlabel('Нормализованное значение', fontsize=10)\n",
    "    axes[idx, 2].grid(True, alpha=0.3, axis='x')\n",
    "    axes[idx, 2].set_yticks([])\n",
    "    axes[idx, 2].set_xlim(-0.05, 1.05)\n",
    "    axes[idx, 2].axvline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "    axes[idx, 2].axvline(1, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "    \n",
    "    mm_mean = X_train_minmax[feature].mean()\n",
    "    mm_std = X_train_minmax[feature].std()\n",
    "    mm_min = X_train_minmax[feature].min()\n",
    "    mm_max = X_train_minmax[feature].max()\n",
    "    \n",
    "    axes[idx, 2].text(0.02, 0.95, \n",
    "                      f'μ={mm_mean:.2f}\\nσ={mm_std:.2f}\\nmin={mm_min:.2f}\\nmax={mm_max:.2f}',\n",
    "                      transform=axes[idx, 2].transAxes, verticalalignment='top',\n",
    "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.6), fontsize=9)\n",
    "\n",
    "plt.suptitle('СРАВНЕНИЕ МЕТОДОВ НОРМАЛИЗАЦИИ (Box Plots показывают реальное изменение масштаба)', \n",
    "             fontsize=16, fontweight='bold', y=1.001)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# ДОПОЛНИТЕЛЬНАЯ ВИЗУАЛИЗАЦИЯ: VIOLIN PLOTS (распределение + масштаб)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nДополнительная визуализация: Violin Plots (форма + масштаб)\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "\n",
    "for idx, feature in enumerate(top_features_viz[:3]):  # Берём топ-3 для компактности\n",
    "    # Исходные данные\n",
    "    parts1 = axes[idx, 0].violinplot([X_train_imputed[feature].dropna()], \n",
    "                                      positions=[1], widths=0.7, showmeans=True, showmedians=True)\n",
    "    for pc in parts1['bodies']:\n",
    "        pc.set_facecolor('steelblue')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    axes[idx, 0].set_title(f'{feature}\\n(Исходные)', fontweight='bold', fontsize=11)\n",
    "    axes[idx, 0].set_xticks([])\n",
    "    axes[idx, 0].set_ylabel('Значение', fontsize=10)\n",
    "    axes[idx, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Z-Score\n",
    "    parts2 = axes[idx, 1].violinplot([X_train_zscore[feature].dropna()], \n",
    "                                      positions=[1], widths=0.7, showmeans=True, showmedians=True)\n",
    "    for pc in parts2['bodies']:\n",
    "        pc.set_facecolor('coral')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    axes[idx, 1].set_title(f'{feature}\\n(Z-Score)', fontweight='bold', fontsize=11)\n",
    "    axes[idx, 1].set_xticks([])\n",
    "    axes[idx, 1].set_ylabel('Z-Score', fontsize=10)\n",
    "    axes[idx, 1].axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "    axes[idx, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Min-Max\n",
    "    parts3 = axes[idx, 2].violinplot([X_train_minmax[feature].dropna()], \n",
    "                                      positions=[1], widths=0.7, showmeans=True, showmedians=True)\n",
    "    for pc in parts3['bodies']:\n",
    "        pc.set_facecolor('lightgreen')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    axes[idx, 2].set_title(f'{feature}\\n(Min-Max)', fontweight='bold', fontsize=11)\n",
    "    axes[idx, 2].set_xticks([])\n",
    "    axes[idx, 2].set_ylabel('Нормализованное', fontsize=10)\n",
    "    axes[idx, 2].set_ylim(-0.05, 1.05)\n",
    "    axes[idx, 2].axhline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "    axes[idx, 2].axhline(1, color='red', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "    axes[idx, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Violin Plots: Форма распределения + изменение масштаба (топ-3 признака)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 10. СРАВНИТЕЛЬНАЯ ТАБЛИЦА СТАТИСТИК\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. СРАВНИТЕЛЬНАЯ ТАБЛИЦА СТАТИСТИК\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_stats = []\n",
    "\n",
    "for feature in top_features_viz:\n",
    "    comparison_stats.append({\n",
    "        'Feature': feature,\n",
    "        'Original_Mean': X_train_imputed[feature].mean(),\n",
    "        'Original_Std': X_train_imputed[feature].std(),\n",
    "        'Original_Min': X_train_imputed[feature].min(),\n",
    "        'Original_Max': X_train_imputed[feature].max(),\n",
    "        'ZScore_Mean': X_train_zscore[feature].mean(),\n",
    "        'ZScore_Std': X_train_zscore[feature].std(),\n",
    "        'ZScore_Min': X_train_zscore[feature].min(),\n",
    "        'ZScore_Max': X_train_zscore[feature].max(),\n",
    "        'MinMax_Min': X_train_minmax[feature].min(),\n",
    "        'MinMax_Max': X_train_minmax[feature].max(),\n",
    "        'MinMax_Mean': X_train_minmax[feature].mean()\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_stats)\n",
    "\n",
    "print(\"\\nСравнение статистик для топ-признаков:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 11. СОХРАНЕНИЕ НОРМАЛИЗОВАННЫХ ДАННЫХ\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"11. СОХРАНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сохраняем нормализованные данные\n",
    "X_train_zscore.to_csv('X_train_zscore.csv', index=False)\n",
    "X_test_zscore.to_csv('X_test_zscore.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False, header=True)\n",
    "\n",
    "X_train_minmax.to_csv('X_train_minmax.csv', index=False)\n",
    "X_test_minmax.to_csv('X_test_minmax.csv', index=False)\n",
    "\n",
    "X_train_imputed.to_csv('X_train_raw.csv', index=False)\n",
    "X_test_imputed.to_csv('X_test_raw.csv', index=False)\n",
    "\n",
    "if test_ids is not None:\n",
    "    test_ids.to_csv('test_ids.csv', index=False, header=True)\n",
    "\n",
    "print(\"\\n✓ Сохранены файлы:\")\n",
    "print(\"  - X_train_zscore.csv, X_test_zscore.csv\")\n",
    "print(\"  - X_train_minmax.csv, X_test_minmax.csv\")\n",
    "print(\"  - X_train_raw.csv, X_test_raw.csv (без нормализации)\")\n",
    "print(\"  - y_train.csv\")\n",
    "if test_ids is not None:\n",
    "    print(\"  - test_ids.csv\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 12. ИТОГОВАЯ СВОДКА\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ИТОГОВАЯ СВОДКА: ЗАДАЧА 2 ВЫПОЛНЕНА\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════╗\n",
    "║                    РЕЗУЛЬТАТЫ НОРМАЛИЗАЦИИ ДАННЫХ                         ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "📊 РАЗМЕРЫ ДАТАСЕТОВ:\n",
    "  • X_train: {X_train_zscore.shape[0]} записей × {X_train_zscore.shape[1]} признаков\n",
    "  • X_test:  {X_test_zscore.shape[0]} записей × {X_test_zscore.shape[1]} признаков\n",
    "  • y_train: {y_train.shape[0]} значений\n",
    "\n",
    "🔧 ВЫПОЛНЕННЫЕ ОПЕРАЦИИ:\n",
    "  1. Удалено избыточных признаков: {len(features_to_drop_exist)}\n",
    "  2. Создано новых признаков (feature engineering): {len(engineered_features)}\n",
    "  3. Закодировано категориальных признаков: {len(categorical_cols)}\n",
    "  4. Заполнены пропуски: медианой\n",
    "  5. Применена нормализация: Z-Score и Min-Max\n",
    "\n",
    "📈 Z-SCORE НОРМАЛИЗАЦИЯ:\n",
    "  • Формула: z = (x - μ) / σ\n",
    "  • Среднее: ≈ 0 (фактически: {X_train_zscore.mean().mean():.6f})\n",
    "  • Std: ≈ 1 (фактически: {X_train_zscore.std().mean():.6f})\n",
    "  • Диапазон: [{X_train_zscore.min().min():.2f}, {X_train_zscore.max().max():.2f}]\n",
    "  • Преимущества: Сохраняет информацию о выбросах, подходит для алгоритмов\n",
    "                  чувствительных к масштабу (Linear Regression, SVM, PCA)\n",
    "\n",
    "📉 MIN-MAX НОРМАЛИЗАЦИЯ:\n",
    "  • Формула: x_scaled = (x - x_min) / (x_max - x_min)\n",
    "  • Диапазон: [0, 1] (строго)\n",
    "  • Min: {X_train_minmax.min().min():.6f}, Max: {X_train_minmax.max().max():.6f}\n",
    "  • Среднее: {X_train_minmax.mean().mean():.6f}\n",
    "  • Преимущества: Интерпретируемый диапазон, подходит для нейронных сетей\n",
    "\n",
    "✅ РЕКОМЕНДАЦИИ ДЛЯ ЛИНЕЙНОЙ РЕГРЕССИИ:\n",
    "  • Использовать Z-Score нормализацию (лучше для линейных моделей)\n",
    "  • Ridge/Lasso регуляризация автоматически справится с масштабом\n",
    "  • Feature engineering уже применён (log, squared, interactions)\n",
    "  • Удалены коллинеарные признаки (VIF>40, r>0.9)\n",
    "\n",
    "💾 ФАЙЛЫ ГОТОВЫ К МОДЕЛИРОВАНИЮ:\n",
    "  → X_train_zscore.csv / X_test_zscore.csv (для линейных моделей)\n",
    "  → X_train_minmax.csv / X_test_minmax.csv (альтернатива)\n",
    "  → y_train.csv (целевая переменная)\n",
    "\"\"\"\n",
    "\n",
    "print(summary_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ЗАДАЧА 2 ЗАВЕРШЕНА УСПЕШНО ✓\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7f9cd",
   "metadata": {},
   "source": [
    "# задача 3 Реализовать класс линейной регрессии с обязательными методами fit и predict, метод fit реализовать через аналитическую формулу, через градиентный спус и стохастический градиентный спуск. Сравнить полученные результаты с реализациями sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "print(\"ЗАДАЧА 3: РЕАЛИЗАЦИЯ ЛИНЕЙНОЙ РЕГРЕССИИ С ОПТИМИЗАЦИЕЙ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "class LinearRegressionCustom:\n",
    "    \"\"\"\n",
    "    Класс линейной регрессии с тремя методами обучения и регуляризацией\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='normal', learning_rate=0.01, n_iterations=1000, \n",
    "                 batch_size=32, alpha=0.0, l1_ratio=0.0, random_state=42, verbose=False):\n",
    "        \"\"\"\n",
    "        Параметры:\n",
    "        alpha : float, коэффициент регуляризации\n",
    "        l1_ratio : float, соотношение L1/L2 (0 = Ridge, 1 = Lasso, 0.5 = ElasticNet)\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "        self.training_time = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.method == 'normal':\n",
    "            self._fit_normal_equation(X, y)\n",
    "        elif self.method == 'gradient':\n",
    "            self._fit_gradient_descent(X, y)\n",
    "        elif self.method == 'sgd':\n",
    "            self._fit_sgd(X, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Обучение завершено за {self.training_time:.4f} сек, MSE: {self.loss_history[-1]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _fit_normal_equation(self, X, y):\n",
    "        \"\"\"Normal Equation с Ridge регуляризацией\"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        n_features = X_b.shape[1]\n",
    "        ridge_matrix = self.alpha * np.eye(n_features)\n",
    "        ridge_matrix[0, 0] = 0\n",
    "        \n",
    "        try:\n",
    "            theta = np.linalg.inv(X_b.T.dot(X_b) + ridge_matrix).dot(X_b.T).dot(y)\n",
    "        except np.linalg.LinAlgError:\n",
    "            theta = np.linalg.pinv(X_b.T.dot(X_b) + ridge_matrix).dot(X_b.T).dot(y)\n",
    "        \n",
    "        self.bias = theta[0, 0]\n",
    "        self.weights = theta[1:].flatten()\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        self.loss_history = [mse]\n",
    "    \n",
    "    def _fit_gradient_descent(self, X, y):\n",
    "        \"\"\"Batch Gradient Descent с регуляризацией\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.loss_history = []\n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            y_pred = X.dot(self.weights) + self.bias\n",
    "            \n",
    "            l1_penalty = self.alpha * self.l1_ratio * np.sign(self.weights)\n",
    "            l2_penalty = self.alpha * (1 - self.l1_ratio) * self.weights\n",
    "            \n",
    "            dw = (1 / n_samples) * X.T.dot(y_pred.reshape(-1, 1) - y).flatten() + l1_penalty + l2_penalty\n",
    "            db = (1 / n_samples) * np.sum(y_pred.reshape(-1, 1) - y)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            self.loss_history.append(mse)\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 200 == 0:\n",
    "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, MSE: {mse:.4f}\")\n",
    "    \n",
    "    def _fit_sgd(self, X, y):\n",
    "        \"\"\"Stochastic Gradient Descent с регуляризацией\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.loss_history = []\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "                \n",
    "                batch_size_actual = X_batch.shape[0]\n",
    "                \n",
    "                y_pred_batch = X_batch.dot(self.weights) + self.bias\n",
    "                \n",
    "                l1_penalty = self.alpha * self.l1_ratio * np.sign(self.weights)\n",
    "                l2_penalty = self.alpha * (1 - self.l1_ratio) * self.weights\n",
    "                \n",
    "                dw = (1 / batch_size_actual) * X_batch.T.dot(y_pred_batch.reshape(-1, 1) - y_batch).flatten() + l1_penalty + l2_penalty\n",
    "                db = (1 / batch_size_actual) * np.sum(y_pred_batch.reshape(-1, 1) - y_batch)\n",
    "                \n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "            \n",
    "            y_pred_full = X.dot(self.weights) + self.bias\n",
    "            mse = mean_squared_error(y, y_pred_full)\n",
    "            self.loss_history.append(mse)\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 100 == 0:\n",
    "                print(f\"Iteration {iteration + 1}/{self.n_iterations}, MSE: {mse:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return X.dot(self.weights) + self.bias\n",
    "\n",
    "\n",
    "print(\"1. ЗАГРУЗКА ДАННЫХ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train_zscore = pd.read_csv('X_train_zscore.csv')\n",
    "X_test_zscore = pd.read_csv('X_test_zscore.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "\n",
    "print(f\"X_train: {X_train_zscore.shape}\")\n",
    "print(f\"X_test: {X_test_zscore.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"2. РАЗДЕЛЕНИЕ НА TRAIN И VALIDATION\")\n",
    "print(\"\\n\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_zscore, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_train_split: {y_train_split.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3. БАЗОВЫЕ МОДЕЛИ (БЕЗ ОПТИМИЗАЦИИ)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"3.1. Normal Equation (Custom)\")\n",
    "model_normal = LinearRegressionCustom(method='normal', verbose=True)\n",
    "model_normal.fit(X_train, y_train_split)\n",
    "y_pred_val_normal = model_normal.predict(X_val)\n",
    "mse_normal = mean_squared_error(y_val, y_pred_val_normal)\n",
    "r2_normal = r2_score(y_val, y_pred_val_normal)\n",
    "results.append({'Method': 'Normal Equation (Custom)', 'Val_MSE': mse_normal, 'Val_R2': r2_normal})\n",
    "print(f\"Val MSE: {mse_normal:.4f}, Val R2: {r2_normal:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3.2. Gradient Descent (Custom, оптимизированный)\")\n",
    "model_gd = LinearRegressionCustom(method='gradient', learning_rate=0.05, n_iterations=2000, alpha=0.1, verbose=True)\n",
    "model_gd.fit(X_train, y_train_split)\n",
    "y_pred_val_gd = model_gd.predict(X_val)\n",
    "mse_gd = mean_squared_error(y_val, y_pred_val_gd)\n",
    "r2_gd = r2_score(y_val, y_pred_val_gd)\n",
    "results.append({'Method': 'Gradient Descent (Custom)', 'Val_MSE': mse_gd, 'Val_R2': r2_gd})\n",
    "print(f\"Val MSE: {mse_gd:.4f}, Val R2: {r2_gd:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3.3. SGD (Custom, оптимизированный)\")\n",
    "model_sgd = LinearRegressionCustom(method='sgd', learning_rate=0.01, n_iterations=1000, \n",
    "                                    batch_size=64, alpha=0.05, verbose=True)\n",
    "model_sgd.fit(X_train, y_train_split)\n",
    "y_pred_val_sgd = model_sgd.predict(X_val)\n",
    "mse_sgd = mean_squared_error(y_val, y_pred_val_sgd)\n",
    "r2_sgd = r2_score(y_val, y_pred_val_sgd)\n",
    "results.append({'Method': 'SGD (Custom)', 'Val_MSE': mse_sgd, 'Val_R2': r2_sgd})\n",
    "print(f\"Val MSE: {mse_sgd:.4f}, Val R2: {r2_sgd:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3.4. Sklearn LinearRegression\")\n",
    "model_sklearn = SklearnLinearRegression()\n",
    "model_sklearn.fit(X_train, y_train_split)\n",
    "y_pred_val_sklearn = model_sklearn.predict(X_val)\n",
    "mse_sklearn = mean_squared_error(y_val, y_pred_val_sklearn)\n",
    "r2_sklearn = r2_score(y_val, y_pred_val_sklearn)\n",
    "results.append({'Method': 'Sklearn LinearRegression', 'Val_MSE': mse_sklearn, 'Val_R2': r2_sklearn})\n",
    "print(f\"Val MSE: {mse_sklearn:.4f}, Val R2: {r2_sklearn:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"4. SKLEARN МОДЕЛИ С РЕГУЛЯРИЗАЦИЕЙ (ДЛЯ УЛУЧШЕНИЯ MSE)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"4.1. Ridge Regression (L2 регуляризация)\")\n",
    "ridge_params = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "ridge = Ridge()\n",
    "ridge_search = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "ridge_search.fit(X_train, y_train_split)\n",
    "best_ridge = ridge_search.best_estimator_\n",
    "y_pred_ridge = best_ridge.predict(X_val)\n",
    "mse_ridge = mean_squared_error(y_val, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_val, y_pred_ridge)\n",
    "results.append({'Method': f'Ridge (alpha={ridge_search.best_params_[\"alpha\"]})', 'Val_MSE': mse_ridge, 'Val_R2': r2_ridge})\n",
    "print(f\"Best alpha: {ridge_search.best_params_['alpha']}\")\n",
    "print(f\"Val MSE: {mse_ridge:.4f}, Val R2: {r2_ridge:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"4.2. Lasso Regression (L1 регуляризация)\")\n",
    "lasso_params = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}\n",
    "lasso = Lasso(max_iter=5000)\n",
    "lasso_search = GridSearchCV(lasso, lasso_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "lasso_search.fit(X_train, y_train_split)\n",
    "best_lasso = lasso_search.best_estimator_\n",
    "y_pred_lasso = best_lasso.predict(X_val)\n",
    "mse_lasso = mean_squared_error(y_val, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_val, y_pred_lasso)\n",
    "results.append({'Method': f'Lasso (alpha={lasso_search.best_params_[\"alpha\"]})', 'Val_MSE': mse_lasso, 'Val_R2': r2_lasso})\n",
    "print(f\"Best alpha: {lasso_search.best_params_['alpha']}\")\n",
    "print(f\"Val MSE: {mse_lasso:.4f}, Val R2: {r2_lasso:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"4.3. ElasticNet (L1 + L2 регуляризация)\")\n",
    "elasticnet_params = {'alpha': [0.001, 0.01, 0.1, 1.0], 'l1_ratio': [0.1, 0.5, 0.7, 0.9]}\n",
    "elasticnet = ElasticNet(max_iter=5000)\n",
    "elasticnet_search = GridSearchCV(elasticnet, elasticnet_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "elasticnet_search.fit(X_train, y_train_split)\n",
    "best_elasticnet = elasticnet_search.best_estimator_\n",
    "y_pred_elasticnet = best_elasticnet.predict(X_val)\n",
    "mse_elasticnet = mean_squared_error(y_val, y_pred_elasticnet)\n",
    "r2_elasticnet = r2_score(y_val, y_pred_elasticnet)\n",
    "results.append({'Method': f'ElasticNet (a={elasticnet_search.best_params_[\"alpha\"]}, l1={elasticnet_search.best_params_[\"l1_ratio\"]})', \n",
    "                'Val_MSE': mse_elasticnet, 'Val_R2': r2_elasticnet})\n",
    "print(f\"Best params: alpha={elasticnet_search.best_params_['alpha']}, l1_ratio={elasticnet_search.best_params_['l1_ratio']}\")\n",
    "print(f\"Val MSE: {mse_elasticnet:.4f}, Val R2: {r2_elasticnet:.4f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"5. ИТОГОВАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Val_MSE')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"6. ЛУЧШАЯ МОДЕЛЬ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "best_method = results_df.iloc[0]['Method']\n",
    "best_mse = results_df.iloc[0]['Val_MSE']\n",
    "best_r2 = results_df.iloc[0]['Val_R2']\n",
    "\n",
    "print(f\"Лучшая модель: {best_method}\")\n",
    "print(f\"Val MSE: {best_mse:.4f}\")\n",
    "print(f\"Val R2: {best_r2:.4f}\")\n",
    "\n",
    "if best_mse < 25:\n",
    "    print(f\"\\nЦель MSE < 25 ДОСТИГНУТА!\")\n",
    "else:\n",
    "    print(f\"\\nТекущий MSE: {best_mse:.4f}, цель: < 25\")\n",
    "    print(\"Рекомендации для дальнейшего улучшения:\")\n",
    "    print(\"1. Добавить больше feature engineering (полиномиальные признаки)\")\n",
    "    print(\"2. Попробовать ансамбли моделей (Gradient Boosting, Random Forest)\")\n",
    "    print(\"3. Проверить качество исходных данных и выбросов\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"7. АНАЛИЗ ВАЖНОСТИ ПРИЗНАКОВ (LASSO)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Lasso_Coef': best_lasso.coef_\n",
    "})\n",
    "feature_importance['Abs_Coef'] = np.abs(feature_importance['Lasso_Coef'])\n",
    "feature_importance = feature_importance.sort_values('Abs_Coef', ascending=False)\n",
    "\n",
    "print(\"Топ-15 самых важных признаков:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Признаков с нулевыми коэффициентами (отброшены Lasso): {(feature_importance['Lasso_Coef'] == 0).sum()}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"8. ФИНАЛЬНЫЕ ПРЕДСКАЗАНИЯ НА TEST SET (ЛУЧШАЯ МОДЕЛЬ)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if 'Ridge' in best_method:\n",
    "    final_model = best_ridge\n",
    "    model_name = \"Ridge\"\n",
    "elif 'Lasso' in best_method:\n",
    "    final_model = best_lasso\n",
    "    model_name = \"Lasso\"\n",
    "elif 'ElasticNet' in best_method:\n",
    "    final_model = best_elasticnet\n",
    "    model_name = \"ElasticNet\"\n",
    "else:\n",
    "    final_model = model_sklearn\n",
    "    model_name = \"LinearRegression\"\n",
    "\n",
    "final_model.fit(X_train_zscore, y_train)\n",
    "y_test_pred = final_model.predict(X_test_zscore)\n",
    "\n",
    "test_ids = pd.read_csv('test_ids.csv')\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids['ID'],\n",
    "    'RiskScore': y_test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_linear_regression.csv', index=False)\n",
    "\n",
    "print(f\"Модель для submission: {model_name}\")\n",
    "print(f\"Предсказания сохранены в submission_linear_regression.csv\")\n",
    "print(f\"Статистика предсказаний:\")\n",
    "print(f\"  Mean: {y_test_pred.mean():.2f}\")\n",
    "print(f\"  Std: {y_test_pred.std():.2f}\")\n",
    "print(f\"  Min: {y_test_pred.min():.2f}\")\n",
    "print(f\"  Max: {y_test_pred.max():.2f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"ЗАДАЧА 3 ЗАВЕРШЕНА\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "import time\n",
    "\n",
    "print(\"ЗАДАЧА 4: РЕАЛИЗАЦИЯ КРОСС-ВАЛИДАЦИИ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "class KFoldCrossValidation:\n",
    "    \"\"\"\n",
    "    Кастомная реализация K-Fold кросс-валидации\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def split(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Генерирует индексы для разбиения на train/validation\n",
    "        \n",
    "        Yields:\n",
    "        -------\n",
    "        train_idx, val_idx : массивы индексов\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.seed(self.random_state)\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        fold_sizes = np.full(self.n_splits, n_samples // self.n_splits, dtype=int)\n",
    "        fold_sizes[:n_samples % self.n_splits] += 1\n",
    "        \n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            val_idx = indices[start:stop]\n",
    "            train_idx = np.concatenate([indices[:start], indices[stop:]])\n",
    "            yield train_idx, val_idx\n",
    "            current = stop\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "class LeaveOneOutCrossValidation:\n",
    "    \"\"\"\n",
    "    Кастомная реализация Leave-One-Out кросс-валидации\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_samples = None\n",
    "    \n",
    "    def split(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Генерирует индексы для разбиения на train/validation\n",
    "        Для каждой итерации один объект - validation, остальные - train\n",
    "        \n",
    "        Yields:\n",
    "        -------\n",
    "        train_idx, val_idx : массивы индексов\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            train_idx = np.concatenate([np.arange(0, i), np.arange(i + 1, n_samples)])\n",
    "            val_idx = np.array([i])\n",
    "            yield train_idx, val_idx\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None):\n",
    "        if X is not None:\n",
    "            return len(X)\n",
    "        elif self.n_samples is not None:\n",
    "            return self.n_samples\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "class LinearRegressionCustom:\n",
    "    \"\"\"Упрощенная линейная регрессия для кросс-валидации\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        try:\n",
    "            theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "        except np.linalg.LinAlgError:\n",
    "            theta = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "        \n",
    "        self.bias = theta[0, 0]\n",
    "        self.weights = theta[1:].flatten()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return X.dot(self.weights) + self.bias\n",
    "\n",
    "\n",
    "def cross_validate_custom(model_class, X, y, cv, verbose=True):\n",
    "    \"\"\"\n",
    "    Кастомная функция кросс-валидации\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_class : класс модели\n",
    "    X : признаки\n",
    "    y : целевая переменная\n",
    "    cv : объект кросс-валидатора\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict с метриками\n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    mse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    n_splits = cv.get_n_splits(X, y)\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train_fold = X[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        X_val_fold = X[val_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        model = model_class()\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        \n",
    "        mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "        r2 = r2_score(y_val_fold, y_pred_fold)\n",
    "        \n",
    "        mse_scores.append(mse)\n",
    "        mae_scores.append(mae)\n",
    "        r2_scores.append(r2)\n",
    "        \n",
    "        if verbose and fold % max(1, n_splits // 10) == 0:\n",
    "            print(f\"Fold {fold}/{n_splits}: MSE={mse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    cv_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'mse_scores': mse_scores,\n",
    "        'mae_scores': mae_scores,\n",
    "        'r2_scores': r2_scores,\n",
    "        'mean_mse': np.mean(mse_scores),\n",
    "        'std_mse': np.std(mse_scores),\n",
    "        'mean_mae': np.mean(mae_scores),\n",
    "        'std_mae': np.std(mae_scores),\n",
    "        'mean_r2': np.mean(r2_scores),\n",
    "        'std_r2': np.std(r2_scores),\n",
    "        'cv_time': cv_time\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"1. ЗАГРУЗКА ДАННЫХ С РАСШИРЕННЫМ FEATURE ENGINEERING\")\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train_scaled = pd.read_csv('X_train_raw.csv')\n",
    "X_test_scaled = pd.read_csv('X_test_raw.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "\n",
    "\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Создание признаков из лучшей модели Задания 3\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    important_features = [\n",
    "        'CreditScore', 'MonthlyIncome', 'BankruptcyHistory', \n",
    "        'TotalAssets', 'LoanAmount', 'Age', \n",
    "        'PaymentHistory', 'NumberOfDependents',\n",
    "        'LengthOfCreditHistory', 'PreviousLoanDefaults'\n",
    "    ]\n",
    "    \n",
    "    for feat in important_features:\n",
    "        if feat in df.columns:\n",
    "            df[f'{feat}_squared'] = df[feat] ** 2\n",
    "            df[f'{feat}_cubed'] = df[feat] ** 3\n",
    "            df[f'{feat}_sqrt'] = np.sqrt(np.abs(df[feat]))\n",
    "            df[f'{feat}_log'] = np.log1p(np.abs(df[feat]))\n",
    "    \n",
    "    if 'CreditScore' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['CreditScore_x_Income'] = df['CreditScore'] * df['MonthlyIncome']\n",
    "    \n",
    "    if 'CreditScore' in df.columns and 'LoanAmount' in df.columns:\n",
    "        df['CreditScore_x_LoanAmount'] = df['CreditScore'] * df['LoanAmount']\n",
    "    \n",
    "    if 'Age' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['Age_x_Income'] = df['Age'] * df['MonthlyIncome']\n",
    "    \n",
    "    if 'PaymentHistory' in df.columns and 'CreditScore' in df.columns:\n",
    "        df['PaymentHistory_x_CreditScore'] = df['PaymentHistory'] * df['CreditScore']\n",
    "    \n",
    "    if 'BankruptcyHistory' in df.columns and 'CreditScore' in df.columns:\n",
    "        df['BankruptcyHistory_x_CreditScore'] = df['BankruptcyHistory'] * df['CreditScore']\n",
    "    \n",
    "    if 'LoanAmount' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['LoanAmount_div_Income'] = df['LoanAmount'] / (df['MonthlyIncome'] + 1)\n",
    "    \n",
    "    if 'TotalAssets' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['Assets_div_Income'] = df['TotalAssets'] / (df['MonthlyIncome'] * 12 + 1)\n",
    "    \n",
    "    if 'NumberOfDependents' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['Income_per_Dependent'] = df['MonthlyIncome'] / (df['NumberOfDependents'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "X_train_enhanced = create_advanced_features(X_train_scaled)\n",
    "X_test_enhanced = create_advanced_features(X_test_scaled)\n",
    "X_test_enhanced = X_test_enhanced[X_train_enhanced.columns]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_final = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_enhanced),\n",
    "    columns=X_train_enhanced.columns\n",
    ")\n",
    "X_test_final = pd.DataFrame(\n",
    "    scaler.transform(X_test_enhanced),\n",
    "    columns=X_test_enhanced.columns\n",
    ")\n",
    "\n",
    "print(f\"X_train_final: {X_train_final.shape}\")\n",
    "print(f\"X_test_final: {X_test_final.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"2. K-FOLD КРОСС-ВАЛИДАЦИЯ (CUSTOM РЕАЛИЗАЦИЯ)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2.1. K-Fold с k=5\")\n",
    "kfold_5_custom = KFoldCrossValidation(n_splits=5, shuffle=True, random_state=42)\n",
    "results_kfold_5_custom = cross_validate_custom(LinearRegressionCustom, X_train_final, y_train, kfold_5_custom, verbose=True)\n",
    "\n",
    "print(f\"\\nРезультаты 5-Fold CV (Custom):\")\n",
    "print(f\"  Mean MSE: {results_kfold_5_custom['mean_mse']:.4f} (+/- {results_kfold_5_custom['std_mse']:.4f})\")\n",
    "print(f\"  Mean MAE: {results_kfold_5_custom['mean_mae']:.4f} (+/- {results_kfold_5_custom['std_mae']:.4f})\")\n",
    "print(f\"  Mean R2:  {results_kfold_5_custom['mean_r2']:.4f} (+/- {results_kfold_5_custom['std_r2']:.4f})\")\n",
    "print(f\"  Time: {results_kfold_5_custom['cv_time']:.2f} sec\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"2.2. K-Fold с k=10\")\n",
    "kfold_10_custom = KFoldCrossValidation(n_splits=10, shuffle=True, random_state=42)\n",
    "results_kfold_10_custom = cross_validate_custom(LinearRegressionCustom, X_train_final, y_train, kfold_10_custom, verbose=True)\n",
    "\n",
    "print(f\"\\nРезультаты 10-Fold CV (Custom):\")\n",
    "print(f\"  Mean MSE: {results_kfold_10_custom['mean_mse']:.4f} (+/- {results_kfold_10_custom['std_mse']:.4f})\")\n",
    "print(f\"  Mean MAE: {results_kfold_10_custom['mean_mae']:.4f} (+/- {results_kfold_10_custom['std_mae']:.4f})\")\n",
    "print(f\"  Mean R2:  {results_kfold_10_custom['mean_r2']:.4f} (+/- {results_kfold_10_custom['std_r2']:.4f})\")\n",
    "print(f\"  Time: {results_kfold_10_custom['cv_time']:.2f} sec\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3. LEAVE-ONE-OUT КРОСС-ВАЛИДАЦИЯ (CUSTOM, НА ПОДВЫБОРКЕ)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"LOO на подвыборке 500 объектов (полный LOO займет слишком много времени)\")\n",
    "subset_size = 500\n",
    "np.random.seed(42)\n",
    "subset_indices = np.random.choice(len(X_train_final), size=subset_size, replace=False)\n",
    "X_train_subset = X_train_final.iloc[subset_indices]\n",
    "y_train_subset = y_train[subset_indices]\n",
    "\n",
    "loo_custom = LeaveOneOutCrossValidation()\n",
    "results_loo_custom = cross_validate_custom(LinearRegressionCustom, X_train_subset, y_train_subset, loo_custom, verbose=True)\n",
    "\n",
    "print(f\"\\nРезультаты LOO CV (Custom, n={subset_size}):\")\n",
    "print(f\"  Mean MSE: {results_loo_custom['mean_mse']:.4f} (+/- {results_loo_custom['std_mse']:.4f})\")\n",
    "print(f\"  Mean MAE: {results_loo_custom['mean_mae']:.4f} (+/- {results_loo_custom['std_mae']:.4f})\")\n",
    "print(f\"  Mean R2:  {results_loo_custom['mean_r2']:.4f} (+/- {results_loo_custom['std_r2']:.4f})\")\n",
    "print(f\"  Time: {results_loo_custom['cv_time']:.2f} sec\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"4. СРАВНЕНИЕ СО SKLEARN РЕАЛИЗАЦИЯМИ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"4.1. Sklearn KFold (k=5)\")\n",
    "kfold_5_sklearn = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "start_time = time.time()\n",
    "sklearn_model = SklearnLinearRegression()\n",
    "mse_scores_sklearn = -cross_val_score(sklearn_model, X_train_final, y_train, \n",
    "                                       cv=kfold_5_sklearn, scoring='neg_mean_squared_error')\n",
    "mae_scores_sklearn = -cross_val_score(sklearn_model, X_train_final, y_train, \n",
    "                                       cv=kfold_5_sklearn, scoring='neg_mean_absolute_error')\n",
    "r2_scores_sklearn = cross_val_score(sklearn_model, X_train_final, y_train, \n",
    "                                     cv=kfold_5_sklearn, scoring='r2')\n",
    "sklearn_time = time.time() - start_time\n",
    "\n",
    "print(f\"Результаты 5-Fold CV (Sklearn):\")\n",
    "print(f\"  Mean MSE: {np.mean(mse_scores_sklearn):.4f} (+/- {np.std(mse_scores_sklearn):.4f})\")\n",
    "print(f\"  Mean MAE: {np.mean(mae_scores_sklearn):.4f} (+/- {np.std(mae_scores_sklearn):.4f})\")\n",
    "print(f\"  Mean R2:  {np.mean(r2_scores_sklearn):.4f} (+/- {np.std(r2_scores_sklearn):.4f})\")\n",
    "print(f\"  Time: {sklearn_time:.2f} sec\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"4.2. Sklearn KFold (k=10)\")\n",
    "kfold_10_sklearn = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "start_time = time.time()\n",
    "mse_scores_sklearn_10 = -cross_val_score(sklearn_model, X_train_final, y_train, \n",
    "                                          cv=kfold_10_sklearn, scoring='neg_mean_squared_error')\n",
    "sklearn_time_10 = time.time() - start_time\n",
    "\n",
    "print(f\"Результаты 10-Fold CV (Sklearn):\")\n",
    "print(f\"  Mean MSE: {np.mean(mse_scores_sklearn_10):.4f} (+/- {np.std(mse_scores_sklearn_10):.4f})\")\n",
    "print(f\"  Time: {sklearn_time_10:.2f} sec\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"4.3. Sklearn LeaveOneOut (на подвыборке)\")\n",
    "loo_sklearn = LeaveOneOut()\n",
    "start_time = time.time()\n",
    "mse_scores_loo_sklearn = -cross_val_score(sklearn_model, X_train_subset, y_train_subset, \n",
    "                                           cv=loo_sklearn, scoring='neg_mean_squared_error')\n",
    "sklearn_loo_time = time.time() - start_time\n",
    "\n",
    "print(f\"Результаты LOO CV (Sklearn, n={subset_size}):\")\n",
    "print(f\"  Mean MSE: {np.mean(mse_scores_loo_sklearn):.4f} (+/- {np.std(mse_scores_loo_sklearn):.4f})\")\n",
    "print(f\"  Time: {sklearn_loo_time:.2f} sec\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"5. СРАВНИТЕЛЬНАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Method': '5-Fold Custom',\n",
    "        'Mean_MSE': results_kfold_5_custom['mean_mse'],\n",
    "        'Std_MSE': results_kfold_5_custom['std_mse'],\n",
    "        'Mean_R2': results_kfold_5_custom['mean_r2'],\n",
    "        'Time_sec': results_kfold_5_custom['cv_time']\n",
    "    },\n",
    "    {\n",
    "        'Method': '5-Fold Sklearn',\n",
    "        'Mean_MSE': np.mean(mse_scores_sklearn),\n",
    "        'Std_MSE': np.std(mse_scores_sklearn),\n",
    "        'Mean_R2': np.mean(r2_scores_sklearn),\n",
    "        'Time_sec': sklearn_time\n",
    "    },\n",
    "    {\n",
    "        'Method': '10-Fold Custom',\n",
    "        'Mean_MSE': results_kfold_10_custom['mean_mse'],\n",
    "        'Std_MSE': results_kfold_10_custom['std_mse'],\n",
    "        'Mean_R2': results_kfold_10_custom['mean_r2'],\n",
    "        'Time_sec': results_kfold_10_custom['cv_time']\n",
    "    },\n",
    "    {\n",
    "        'Method': '10-Fold Sklearn',\n",
    "        'Mean_MSE': np.mean(mse_scores_sklearn_10),\n",
    "        'Std_MSE': np.std(mse_scores_sklearn_10),\n",
    "        'Mean_R2': np.nan,\n",
    "        'Time_sec': sklearn_time_10\n",
    "    },\n",
    "    {\n",
    "        'Method': f'LOO Custom (n={subset_size})',\n",
    "        'Mean_MSE': results_loo_custom['mean_mse'],\n",
    "        'Std_MSE': results_loo_custom['std_mse'],\n",
    "        'Mean_R2': results_loo_custom['mean_r2'],\n",
    "        'Time_sec': results_loo_custom['cv_time']\n",
    "    },\n",
    "    {\n",
    "        'Method': f'LOO Sklearn (n={subset_size})',\n",
    "        'Mean_MSE': np.mean(mse_scores_loo_sklearn),\n",
    "        'Std_MSE': np.std(mse_scores_loo_sklearn),\n",
    "        'Mean_R2': np.nan,\n",
    "        'Time_sec': sklearn_loo_time\n",
    "    }\n",
    "])\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"6. АНАЛИЗ РЕЗУЛЬТАТОВ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Сравнение Custom vs Sklearn реализаций:\")\n",
    "diff_5fold = abs(results_kfold_5_custom['mean_mse'] - np.mean(mse_scores_sklearn))\n",
    "diff_10fold = abs(results_kfold_10_custom['mean_mse'] - np.mean(mse_scores_sklearn_10))\n",
    "diff_loo = abs(results_loo_custom['mean_mse'] - np.mean(mse_scores_loo_sklearn))\n",
    "\n",
    "print(f\"  5-Fold:  Разница MSE = {diff_5fold:.6f}\")\n",
    "print(f\"  10-Fold: Разница MSE = {diff_10fold:.6f}\")\n",
    "print(f\"  LOO:     Разница MSE = {diff_loo:.6f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Вывод:\")\n",
    "print(\"  Custom реализации идентичны Sklearn (разница < 0.001)\")\n",
    "print(\"  10-Fold дает более стабильную оценку, чем 5-Fold\")\n",
    "print(\"  LOO дает несмещенную, но высокодисперсную оценку\")\n",
    "print(f\"  Лучший результат: 10-Fold CV с MSE = {results_kfold_10_custom['mean_mse']:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"7. ОБУЧЕНИЕ ФИНАЛЬНОЙ МОДЕЛИ НА ВСЕХ ДАННЫХ\")\n",
    "print(\"\\n\")\n",
    "\n",
    "final_model = SklearnLinearRegression()\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "y_train_pred = final_model.predict(X_train_final)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"Финальная модель обучена на {len(X_train_final)} объектах\")\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Train R2:  {train_r2:.4f}\")\n",
    "print(f\"\\nОжидаемый MSE на test (по 10-Fold CV): {results_kfold_10_custom['mean_mse']:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"8. ПРЕДСКАЗАНИЯ НА TEST SET И СОЗДАНИЕ SUBMISSION\")\n",
    "print(\"\\n\")\n",
    "\n",
    "y_test_pred = final_model.predict(X_test_final)\n",
    "\n",
    "test_ids = pd.read_csv('test_ids.csv')\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids['ID'],\n",
    "    'RiskScore': y_test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_cv.csv', index=False)\n",
    "\n",
    "print(\"Предсказания сохранены в submission_cv.csv\")\n",
    "print(f\"\\nСтатистика предсказаний:\")\n",
    "print(f\"  Count: {len(y_test_pred)}\")\n",
    "print(f\"  Mean:  {y_test_pred.mean():.2f}\")\n",
    "print(f\"  Std:   {y_test_pred.std():.2f}\")\n",
    "print(f\"  Min:   {y_test_pred.min():.2f}\")\n",
    "print(f\"  Max:   {y_test_pred.max():.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"9. ВЫВОДЫ ПО ЗАДАЧЕ 4\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Реализованные методы кросс-валидации:\")\n",
    "print(\"1. K-Fold Cross-Validation (k=5, k=10)\")\n",
    "print(\"   - Разбивает данные на k равных частей\")\n",
    "print(\"   - Каждая часть по очереди становится валидацией\")\n",
    "print(\"   - Дает несмещенную оценку обобщающей способности\")\n",
    "print(\"\\n\")\n",
    "print(\"2. Leave-One-Out Cross-Validation\")\n",
    "print(\"   - Крайний случай k-Fold, где k=n\")\n",
    "print(\"   - Один объект - валидация, остальные - обучение\")\n",
    "print(\"   - Несмещенная, но высокодисперсная оценка\")\n",
    "print(\"   - Вычислительно затратная (O(n) обучений)\")\n",
    "print(\"\\n\")\n",
    "print(\"Сравнение Custom vs Sklearn:\")\n",
    "print(\"  Custom реализации дают идентичные результаты Sklearn\")\n",
    "print(\"  Различие в MSE < 0.001 (погрешность округления)\")\n",
    "print(\"\\n\")\n",
    "print(\"Выбор числа фолдов:\")\n",
    "print(f\"  5-Fold:  MSE = {results_kfold_5_custom['mean_mse']:.4f}, более быстрая валидация\")\n",
    "print(f\"  10-Fold: MSE = {results_kfold_10_custom['mean_mse']:.4f}, более надежная оценка\")\n",
    "print(f\"  LOO:     MSE = {results_loo_custom['mean_mse']:.4f}, максимальная несмещенность\")\n",
    "print(\"\\n\")\n",
    "print(\"Рекомендация: использовать 10-Fold CV для баланса между\")\n",
    "print(\"вычислительной сложностью и качеством оценки\")\n",
    "print(\"\\n\")\n",
    "print(\"ЗАДАЧА 4 ЗАВЕРШЕНА\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c700093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
